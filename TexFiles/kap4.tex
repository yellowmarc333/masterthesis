\documentclass[a4paper,11pt]{article}

\parindent0cm
\usepackage
[backend=biber,style=apa,sorting=nyt]
{biblatex}
\addbibresource{literature.bib}

\makeatletter
\newcommand\notsotiny{\@setfontsize\notsotiny{8}{8}}
\newcommand\micro{\@setfontsize\notsotiny{4.5}{4.5}}
\newcommand\middletiny{\@setfontsize\notsotiny{6}{6}}
\makeatother

\usepackage{numprint}
\npthousandsep{\,}

\usepackage{float}
\usepackage[utf8]{inputenc}
\usepackage{csquotes}
\usepackage{array}
\usepackage{dirtytalk}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{multirow}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{color}
\usepackage{layouts}
% printing the textsize used
% \printinunitsof{cm}
% \prntlen{\textwidth}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{tabularx}
\usepackage{graphicx}
\usepackage{pdfpages}
\usepackage[ngerman]{babel}
\usepackage[left=3cm,right=2.5cm,top=2cm,bottom=2cm]{geometry}
\renewcommand{\baselinestretch}{1.5}\normalsize % Zeilenabstand 1.5




\begin{document}

\section{Statistische Auswertung}\label{Kap:statAus}

In diesem Kapitel werden die in Kapitel \ref{kap:machineLearning} vorgestellten \textit{Machine Learning} Modelle auf die verschiedenen numerischen Repräsentationen (Kapitel \ref{kap:3.1Wordemb}) der Nachrichten-Schlagzeilen angewandt. Dieses Kapitel ist in $3$ Teile unterteilt. Im ersten Teil wird das Trainings-, Validierungs- und Test-\textit{Framework} beschrieben. Es folgt eine Beschreibung über die Vorauswahl der Modelle. Der zweite Teil behandelt die Evaluation der ausgewählten Modelle anhand der Gütemaße aus Abschnitt \ref{kap:guetemass}. Im dritten Teil werden die Modelle analysiert (todo: vervollständigen)\\
Die Auswertung und Exploration der Daten sowie die Programmierung der Algorithmen erfolgte ausschließlich mit der Statistik-Software \texttt{R} (R Core Team, 2019).

\subsection{Framework und Vorauswahl der Modelle}

Dieses Unterkapitel erläutert das verwendete \textit{Framework} und beschreibt anschließend die stattgefundene Vorauswahl der finalen Modelle, welche in dieser Arbeit verglichen werden.

\subsubsection{Framework}


Dieser Abschnitt erklärt, wie die \numprint{200847} Beobachtungen des gesamten Datensatzes in \\
Trainings-,Test- und Validierungsdaten unterteilt werden.
Die Testdaten sind der Teil der Daten, auf den die finalen Modelle eine Vorhersage der Nachrichtenkategorie treffen und anschließend die Gütemaße aus Kapitel \ref{kap:guetemass} berechnet und evaluiert werden. Wichtig dabei ist, dass die Testdaten den Modellen nie als Input gedient haben. Der Grund dafür ist, dass die Testdaten als Kriterium dafür dienen, wie gut die gelernten Modelle auf ungesehenen Daten generalisieren können. Zur Ermittlung einer geeigneten Größe der Testdaten wird das Kriterium genommen, dass im Mittel $100$ Beobachtungen in der am niedrigsten repräsentierten Nachrichtensparte enthalten sind. Dies resultiert in einer Stichprobe von $\numprint{20005}$ Beobachtungen, was etwa 10 Prozent der gesamten Daten entspricht. 
Mit dieser Größe ist eine aussagekräftige Evaluation möglich und gleichzeitig stehen den Algorithmen eine größtmögliche Anzahl von Beobachtungen zum Training zur Verfügung. Von den verbleibenden $90$ Prozent ($\numprint{180842}$ Datenpunkte) der Daten wurde wiederum eine Stichprobe von $10$ Prozent gezogen (insgesamt $9$ Prozent der gesamten Daten). Auf dieser Stichprobe der Größe $\numprint{18084}$ erfolgt die im nächsten Abschnitt beschriebene Vorauswahl der Algorithmen und Repräsentationen. Die Daten für die Vorauswahl (folglich Vorauswahldaten genannt) werden wiederum in $80$ Prozent Trainingsdaten ($\numprint{14467}$ Datenpunkte) und $20$ Prozent  Validierungsdaten ($\numprint{3617}$ Datenpunkte) geteilt. Auf den Vorauswahldaten werden die verschiedenen Repräsentationen der Wörter, sowohl für Training als auch Validierung erstellt. Dabei werden je nach Art des \textit{Embeddings} einige wenige Datenpunkte entfernt. Der Grund hierfür ist beispielsweise, dass  unter Nutzung von \textit{BOW} nach Entfernung der \textit{Stopwords} einige Beobachtungen keine Wörter mehr enthalten. Im Falle von \textit{GloVe} werden im Zuge des \textit{Paddings} alle Datenpunkte entfernt, die mehr als $W_{max}$ Wörter enthalten.
Letztlich erhalten alle Algorithmen die selben Teilmengen der Vorauswahldaten zum Trainieren und Validieren. Die Struktur der verwendeten neuronalen Netze sowie die Hyperparameter der Verfahren wurde auf den Validierungsdaten für jedes Modell in mehreren Trainings-Durchläufen angepasst.

\subsubsection{Vorauswahl der Modelle}\label{kap:preselection}

Es gibt viele Kombinationen aus Repräsentationen der Wörter und Algorithmen, die auf diese \textit{Embeddings} angewandt werden. In der in diesem Unterabschnitt beschriebenen Vorauswahl wird die Menge der Kombinationen aus \textit{Word Embeddings} und Modellen auf eine Endauswahl reduziert. Die Modelle der Endauswahl werden dann auf den kompletten Trainingsdaten ($\approx 90$ Prozent der Gesamtdaten) trainiert und in Kapitel \ref{kap:evalFinal} auf den Testdaten ($\approx 10$ Prozent der Gesamtdaten) ausgewertet. In die Endauswahl sollen mindestens ein neuronales Netz als auch ein baumbasiertes Verfahren in die Auswahl aufgenommen werden. Des Weiteren ist eine gute Performance im Sinne der Gütemaße aus Kapitel \ref{kap:guetemass} maßgebend. Für die Vorauswahl werden die Gütemaße $Accuracy$, $fscore_M^{(1)}$ und $CE$ betrachtet. Für $Accuracy$ und $fscore_M^{(1)}$ sind hohe Werte wünschenswert, für $CE$ niedrige Werte.
Die auf den Validierungsdaten evaluierten Ergebnisse sind in Tabelle \ref{tab:preselection} dargestellt.\\


\begin{table}[ht]
\begin{center}
\notsotiny
\begin{tabular}{|m{2cm}||m{1.4cm}m{1.4cm}m{1.4cm}m{1.4cm}m{1.4cm}m{1.4cm}m{1.4cm}|}
\hline
 & \multicolumn{7}{c|}{\textbf{Modelle}}  \\
 \hline
  \textbf{\textit{Word}} \newline \textbf{\textit{Embeddings}} & \textit{XGBoost} & \textit{Random Forest} & \textit{MLP} & \textit{CNN \newline Sequenz} & \textit{CNN GloVe} & \textit{Bi-LSTM \newline Sequenz} &  \textit{Bi-LSTM \newline GloVe} \\
 \hline
 \hline
\textit{BOW} & $0.509$ \newline $0.379$ \newline $1.934$& $0.480$ \newline $0.366$ \newline \textcolor{red}{$5.282$} & $0.504$ \newline $0.301$ \newline $2.066$ & -- & -- & -- & -- \\
\hline
\textit{TFIDF} & $0.496$ \newline $0.374$ \newline $1.998$ & $0.450$ \newline $0.311$ \newline \textcolor{orange}{$4.810$} & $0.510$ \newline $0.331$ \newline $2.070$& -- & -- & -- & -- \\
 \hline
\textit{GloVe unsup. 50D} & -- & -- & -- & -- & $0.382$ \newline $0.195$ \newline $2.379$ & & $0.350$ \newline $0.180$ \newline $2.420$ \\
 \hline
\textit{GloVe 50D} & -- & -- & -- & -- & $0.550$ \newline $0.397$ \newline $1.684$ & -- & $0.537$ \newline $0.408$ \newline $1.680$ \\
 \hline
\textit{GloVe 300D} & -- & -- & -- & -- & \textcolor{LimeGreen}{$0.570$} \newline \textcolor{LimeGreen}{$0.409$} \newline \textcolor{LimeGreen}{$1.637$} & -- & \textcolor{ForestGreen}{$0.587$}  \newline \textcolor{ForestGreen}{$0.467$} \newline \textcolor{ForestGreen}{$1.497$} \\
 \hline
\textit{SOW GloVe unsup. 50D} & \textcolor{red}{$0.273$} \newline \textcolor{orange}{$0.125$} \newline $2.714$ & $0.299$ \newline $0.132$ \newline $4.126$ & \textcolor{orange}{$0.287$} \newline \textcolor{red}{$0.078$} \newline $2.643$ & -- & -- & -- & -- \\
 \hline
\textit{SOW GloVe 50D} & $0.436$ \newline $0.289$ \newline $2.091$ & $0.434$ \newline $0.257$ \newline $4.322$ & $0.418$ \newline $0.187$ \newline $2.100$ & & -- & -- & -- \\
 \hline
\textit{SOW GloVe 300D}  & $0.497$ \newline $0.344$ \newline $1.942$ & $0.447$ \newline $0.249$ \newline $4.210$ & $0.499$ \newline $0.260$ \newline $1.882$ & & -- & -- & -- \\
 \hline
\textit{Sequence} & -- & -- & -- & $0.451$ \newline $0.235$ \newline $2.246$ & -- & $0.477$ \newline $0.263$ \newline $2.194$ & -- \\
   \hline
\end{tabular}

  \caption{Ergebnis von $Accuracy$, $fscore_M^{(1)}$ und $CE$ auf den Validierungsdaten für Kombinationen aus \textit{Word Embeddings} und \textit{Machine Learning} Modellen. In grün ist die Kombination mit der jeweils besten Performance bezüglich eines Maßes markiert. Hellgrün ist die zweitbeste Performance, rot die schlechteste und orange die zweitschlechteste.}  
  \label{tab:preselection}
\end{center}
\end{table}

% \cellcolor[HTML]{AA0044}
Das beste Ergebnis bezüglich allen $3$ Gütemaßen erzielt das \textit{Bi-LSTM} neuronale Netz auf dem \textit{GloVe 300D Embedding} mit einer $Accuracy$ von $0.587$, einem $fscore_M^{(1)}$ von $0.467$ und einer $CE$ von $1.497$. Dies bedeutet, dass auf dem Validierungsdatensatz $58.7$ Prozent aller Datenpunkte in der besten Kombination aus \textit{Word Embedding} und \textit{Modell} der richtigen Nachrichtensparte zugeordnet werden kann. Das zweitbeste Modell stellt das \textit{CNN} auf dem selbigen \textit{Embedding} mit einer $Accuracy$ von $0.570$, einem $fscore_M^{(1)}$ von $0.409$ und einer $CE$ von $1.637$. Auf den auf Wikipedia basierenden $GloVe 50D$ Wort-Vektoren erzielen beide Modelle etwas schlechtere Ergebnisse, übertreffen aber die Kennzahlen der restlichen Tabelle. Die auf dem Datensatz gelernten \textit{GloVe unsuper. 50D} \textit{Embeddings} erzielen, von den Embeddings die aus Sequenzen von Wort-Vektoren basieren, die schlechtesten Gütemaße. Bei Betrachtung der Summen dieser Vektoren im \textit{SOW GloVe unsup.50D} Embedding fällt auf, dass \textit{XGBoost} bezüglich der $Accuracy$ ($0.273$) den niedrigsten und bezüglich dem $fscore_M^{(1)}$ ($0.125$) den zweit niedrigsten Wert erzielt. Unter Nutzung dieser Repräsentation der Wörter nimmt das \textit{MLP} für $fscore_M^{(1)}$ ($0.078$) das schlechteste und für $Accuracy$ ($0.287$) das zweit schlechteste Maß an. Im Vergleich der Summen der Sequenzen der Wort-Vektoren, bei denen die Reihenfolge der Wörter nicht mehr einfließt und der Sequenzen der Wort-Vektoren fällt auf, dass letztere Repräsentation, die die Reihenfolge berücksichtigt, in allen Gütemaßen besser abschneidet. Die Repräsentation als Summe erreicht jedoch zumindest bei \textit{SOW GloVe 300D} gute Ergebnisse. Hinsichtlich $Accuracy$ ($0.499$) und $CE$ ($1.882$) schneidet \textit{MLP} am besten ab, in Bezug auf den $fscore_M^{(1)}$ ($0.344$) erzielt \textit{XGBoost} den besten Wert auf diesem \textit{Embedding}. Im Bereich des \textit{BOW}, erzielen sowohl \textit{XGBoost}, \textit{Random Forest} und \textit{MLP} solide Ergebnisse. Darunter schneidet XGBoost mit den Werten $0.509$, $0.378$ und $1.934$ am besten ab. Für das $TFIDF$ \textit{Embedding} sind die Ergebnisse bei \textit{XGBoost} und \textit{Random Forest} etwas schlechter und bei \textit{MLP} etwas besser. Die marginalen Unterschiede sind interessant, da in der Berechnung des \textit{TFIDF-Embedding} zusätzliche Aspekte berücksichtigt wurden, zum Beispiel dass selten vorkommende Wörter höher gewichtet und häufig vorkommende Wörter niedriger gewichtet werden. Die beiden mit Abstand höchsten Ausprägungen der \textit{CE} nimmt \textit{Random Forest} mit $5.282$ bei \textit{BOW} und $4.810$ bei \textit{TFIDF} an. Die hohen Werte auf allen \textit{Word Embeddings} sind dadurch zu erklären, dass \textit{Random Forest} kein Verfahren ist, dass die \textit{CE} als Optimierungskriterium nutzt. Die Repräsentation der Wörter, die eine Sequenz aus Wort-Indizes bilden (\textit{Sequence}) erzielen unter der Anwendung von \textit{CNN} ($Accuracy = 0.451$) und \textit{Bi-LSTM} ($Accuracy = 0.477$) akzeptable Ergebnisse. Auch hier schneidet das \textit{Bi-LSTM} besser als das \textit{CNN} ab. Hierbei werden die Wort-Vektoren mit überwachtem Lernen in einer \textit{Embedding}-Zwischenschicht im neuronalen Netz gelernt. Obwohl das \textit{Sequence} \textit{Embedding} die Reihenfolge der Wörter im Satz berücksichtigt, sind die Ergebnisse schlechter als bei anderen Repräsentationen wie \textit{SOW Glove 300D} oder \textit{BOW}, die die Reihenfolge nicht miteinbeziehen.\\

Zusammenfassend ist in Tabelle \ref{tab:preselection} zu sehen, dass die neuronalen Netze \textit{CNN} und \textit{Bi-LSTM} auf dem \textit{SOW GloVe 300D} die besten Ergebnisse hinsichtlich aller $3$ Gütemaße erzielen. Diese beiden Kombinationen werden deshalb in die Auswahl der finalen Modelle aufgenommen. Auf den klassischen \textit{Embeddings} \textit{BOW} und \textit{TFIDF} erzielen \textit{XGBoost} und \textit{MLP} akzeptable Ergebnisse. Da sich die beiden Repräsentationen hinsichtlich ihrer Struktur wenig unterscheiden, wird ausschließlich ein Modell auf einem der beiden \textit{Embeddings} in die Endauswahl aufgenommen. Dies ist \textit{XGBoost} unter Verwendung von \textit{BOW}, welches für $fscore_M^{(1)}$ und $CE$ das beste und für $Accuracy$ das zweitbeste Resultat erzielt. Ein weiteres Modell der Endselektion ist \textit{MLP} auf \textit{SOW GloVe 300D}, welches von allen Repräsentationen, die auf Summen von Wort-Vektoren basieren, die beste \textit{Accuracy} und \textit{CE} erzielt. \\

Die vier Kombinationen der Endauswahl umfassen nun $4$ Modelle und $3$ Arten der Repräsentationen der Wörter und werden in den nächsten Abschnitten näher vorgestellt.





\subsection{Evaluation und Vergleich der Modelle} \label{kap:evalFinal}

In diesem Abschnitt werden nun die finalen Modelle auf den etwa $90$ Prozent der Gesamtdaten ($180842$ Datenpunkte) trainiert und auf den etwa $10$ Prozent Testdaten ($20005$ Datenpunkte) anhand der resultierenden Modellwahrscheinlichkeiten und den \textit{Confusion Matrizes} evaluiert. Die Ergebnisse sind in Tabelle \ref{tab:finalSelection} zusammengetragen. 


\begin{table}[ht]
\centering
\begin{tabular}{|l||ccccc|}
  \hline
\textit{Embedding}, Modell, (Nummer) & $Accuracy$ & $\overline{Accuracy}$ & $fscore_M^{(1)}$ & $CE$ & $\bar{p}_{IfCorrect}$ \\ 
  \hline
\textit{BOW, XGBoost}, (1) & \textcolor{red}{0.624} & 0.487 & 0.541 & \textcolor{red}{1.447} & 0.653 \\ 
  \textit{GloVe 300D, CNN}, (2) & 0.668 & 0.521 & 0.562 & 1.247 & \textcolor{red}{0.646} \\ 
  \textit{GloVe 300D, Bi-LSTM}, (3) & \textcolor{ForestGreen}{0.689} & \textcolor{ForestGreen}{0.552} & \textcolor{ForestGreen}{0.587} & \textcolor{ForestGreen}{1.124} & \textcolor{ForestGreen}{0.825} \\ 
  \textit{SOW GloVe 300D, MLP}, (4) & 0.625 & \textcolor{red}{0.455} & \textcolor{red}{0.499} & 1.358 & 0.705 \\ 
   \hline
\end{tabular}
\caption{Evaluation der Modelle der finalen Auswahl bezüglich der Gütemaße $Accuracy$, $\overline{Accuracy}$, $fscore_M^{(1)}$, $CE$ und  $\bar{p}_{IfCorrect}$. In dunkelgrün ist das beste Ergebnis pro Gütemaß markiert, in rot das schlechteste}
\label{tab:finalSelection}

\end{table}

Zur Beschreibung aller folgenden Ergebnisse wird aus Gründen der Platzersparnis nur noch der Name des Algorithmus ohne das zugehörige \textit{Word-Embedding} genannt (also zum Beispiel \textit{XGBoost}, statt \textit{BOW, XGBoost}).
In der Tabelle \ref{tab:finalSelection} ist zu sehen, dass das \textit{Bi-LSTM} bezüglich allen betrachteten Gütemaßen am besten abschneidet. Mit einer \textit{Accuracy} von $0.689$ sagt es für die Beobachtungen des Testdatensatzes in $68.9$ Prozent der Fälle die richtige der $32$ Klassen vorher. Werden kleinere Klassen wichtiger angesehen, so erkennt das \textit{Bi-LSTM} im Mittel $55.2$ Beobachtungen pro Sparte und erzielt im Sinne des $fscore_M^{(1)}$ einen Wert von $0.582$. Bezüglich der $CE$, die auch als Verlustfunktion im Trainingsprozess minimiert wird, wird ein Wert von $1.124$ erreicht. Wenn das Modell die richtige Klasse vorhersagt, so ist es sich mit einer mittleren Modellwahrscheinlichkeit von $0.825$ mit Abstand am sichersten. Auch wenn für das in dieser Thesis behandelte Klassifikationsproblem die Fehlklassifikationskosten für Beobachtungen aus kleinen Kategorien höher wären, ist dennoch das \textit{Bi-LSTM} den anderen Modellen vorzuziehen. Das zweitbeste Modell ist das \textit{CNN} in Bezug auf \textit{Accuracy} ($0.668$),  $\overline{Accuracy}$ ($0.521$), $fscore_M^{(1)}$ ($0.562$) und $CE$ ($1.247$). Im Falle einer richtigen Klassifikation ist sich dieses Modell jedoch am unsichersten ($\bar{p}_{IfCorrect} =0.646$). Den dritten Platz in Hinsicht auf $Accuracy$ belegt das \textit{MLP} ($0.625$), dicht gefolgt von $XGBoost$ ($0.624$). Bei den beiden Gütemaßen, bei denen kleinere Klassen eine höhere Wichtigkeit besitzen, schneidet das \textit{MLP} am schlechtesten ab ($\overline{Accuracy} = 0.455$, $fscore_M^{(1)} = 0.499$). \textit{XGBoost} belegt bezüglich der $CE$ den letzten Platz.\\
Nachdem nun die globale Performance auf dem kompletten Testdatensatz untersucht wurde, erfolgt nun eine Betrachtung für die einzelnen Nachrichtensparten. Abbildung \ref{abb:AccByClass} zeigt die $Accuracy$ pro Klasse für alle Modelle der Endauswahl.

\begin{figure}[ht]
    \centering
\includegraphics[width = \textwidth,  keepaspectratio]{Images/FinalSelectionAccByClass.pdf} 
\caption{\textit{Accuracy} je Kategorie für die $4$ Modelle der Endauswahl. Geordnet absteigend von links nach rechts nach Größe der Kategorie im Testdatensatz}
\label{abb:AccByClass}
\end{figure}

Die Grafik ist so zu betrachten, dass die Sparten nach der Anzahl Beobachtungen absteigend von links nach rechts geordnet sind. Es ist sichtbar, dass \textit{Bi-LSTM} in $22$ der $32$ Kategorien die beste $Accuracy$ annimmt und gleichzeitig in keiner Sparte am schlechtesten performt. Das \textit{CNN} belegt in $8$ Kategorien den ersten Platz, unter anderem für die beiden kleinsten Klassen \textit{college} und \textit{education}. Nur in \textit{impact} erreicht das \textit{CNN} die niedrigste Trefferrate.
\textit{XGBoost} schneidet in $12$ Kategorien am schlechtesten ab und in keiner am besten. Das \textit{MLP} performt am besten in der Kategorie \textit{sports}, belegt aber in $16$ der $32$ Sparten den letzten Platz. Besonders schlecht schneidet dieses Modell bei den $5$ kleinsten Kategorien ab. In der Sparte \textit{fifty} erreicht das \textit{MLP} als einziges Modell in einer Kategorie eine Trefferquote von $0$ Prozent. Der Bezug zu den Maßen $\overline{Accuracy}$ ($0.455$) und $fscore_M^{(1)}$ ($0.499)$ für das \textit{MLP} aus Tabelle \ref{tab:finalSelection} kann hergestellt werden, da das \textit{MLP} in vielen Kategorien schlecht und nur in wenigen gut abschneidet. \\
Generell ist in Abbildung \ref{abb:AccByClass} zu beobachten, dass die $2$ größten Kategorien \textit{politics} und \textit{wellness \& healthy living} sowie die Sparten \textit{divorce} und \textit{tech} etwa gleich gut von allen Modellen vorhergesagt werden. Kleine Kategorien schneiden tendenziell schlechter ab, was in den generell niedrigeren Maßen $\overline{Accuracy}$ und $fscore_M^{(1)}$ in Tabelle \ref{tab:finalSelection} resultiert. Die $7$ Kategorien mit den meisten Beobachtungen erzielen unter Verwendung des jeweils besten Modells eine $Accuracy$ zwischen $0.76$ und $0.86$ Prozent. \\
\\
Nachfolgend wird untersucht, wie hoch die Modelle die Wahrscheinlichkeit für die richtige Klasse einschätzen und ob diese Wahrscheinlichkeit proportional zu der Anzahl der korrekt klassifizierten Beobachtungen ist. Dazu werden $50$ (heuristischer Wert) Intervalle $([0, 0.02), [0.02, 0.04), \dots, [0.98, 1]$ gebildet und für jede der $19999$ Beobachtungen im Testdatensatz die Wahrscheinlichkeit des Modells für die richtige Klasse erfasst. Gleichzeitig wird in einer logischen Variable (nimmt $1$ für korrekt und $0$ für inkorrekt an) festgehalten, ob das Modell richtig gelegen hat, was gleichbedeutend dazu ist, dass das Modell keiner anderen Klasse eine höhere Wahrscheinlichkeit zugeordnet hat.
Es erfolgt anschließend die Zuordnung Wahrscheinlichkeiten zu den entsprechenden Intervallen. Für alle Intervalle wird nun der Mittelwert über die binäre Variable gebildet. Dieser Mittelwert ist der Anteil der korrekt klassifizierten Beobachtungen pro Intervall der Modellwahrscheinlichkeit und ist für alle Modelle der Endauswahl in Abbildung \ref{abb:CompareProbVsAcc} dargestellt.


\begin{figure}[ht]
    \centering
\includegraphics[width = \textwidth,  keepaspectratio]{Images/FinalSelectionCompareProbVsAcc.pdf} 
\caption{Anteil korrekt klassifizierter Beobachtungen für Intervalle der Modellwahrscheinlichkeiten der $4$ Modelle der Endauswahl. Die gestrichelte Linie markiert das Verhältnis 1:1}
\label{abb:CompareProbVsAcc}
\end{figure}

Wie die Grafik zu lesen ist sei an einem Beispiel verdeutlicht: Für die Mitte des Intervalls der Modellwahrscheinlichkeit von $0.75$ (dies sind alle Beobachtungen die eine Wahrscheinlichkeit für die richtige Klasse in $(0.74,0.76]$ von dem Modell zugeordnet bekommen haben) liegt im Falle des \textit{Bi-LSTM} der Anteil der korrekt klassifizierten Beobachtungen bei etwa $0.66$. In diesem Fall ist sich das Modell also sicherer, die korrekte Klasse vorherzusagen, als es letztendlich nach Evaluation der Fall ist. 
Da sich der Verlauf für das \textit{Bi-LSTM} größtenteils unter der gestrichelten Linie abzeichnet, spricht dies dafür, dass dieses Modell die Wahrscheinlichkeit der richtigen Klasse überschätzt. Dies kann auch den hohen Wert von $\bar{p}_{IfCorrect}$ ($0.825$) aus Tabelle \ref{tab:finalSelection} erklären; wenn das Modell generell hohe Wahrscheinlichkeiten für die wahre Klassen zuteilt, ist der Mittelwert für alle korrekten Einschätzungen (eine Teilmenge davon) auch hoch. Sowohl bei \textit{XGBoost} als auch bei dem \textit{CNN} tritt der gegenteilige Effekt des Unterschätzens der Wahrscheinlichkeit für die richtige Klasse auf, wobei besonders das \textit{CNN} für die hohen Wahrscheinlichkeiten eine noch höhere Trefferquote besitzt. Das \textit{MLP} bewegt sich nahe an der gestrichelten Linie und ist sich tendenziell genau so sicher, wie es auch korrekt klassifiziert. Die Schwankungen sind im linken Teil der Graphik für alle Modelle höher. Dies ist dadurch zu erklären, dass Modelle mit einer guten Performance seltener niedrige Wahrscheinlichkeiten für die richtige Klasse modellieren. Demnach ist die Anzahl der Beobachtungen für kleine Intervall-Mitten geringer und resultiert in einer höheren Schwankung des Anteils der korrekt klassifizierten Beobachtungen. Aus dem gleichen Grund fehlen auch einige Punkte, da für diese keine Beobachtungen im Intervall zu finden sind. \\
\\
In diesem Unterkapitel wurden die Gütemaße für die Modelle der Endauswahl erläutert und die Sicherheit der Modelle untersucht. 


Im nächsten Abschnitt beschäftigt sich mit der Untersuchung von benachbarten Kategorien. todo: bessere Überleitung finden

\subsection{Nachbarkategorien}

In diesem Abschnitt werden $3$ Aspekte untersucht. Diese beschäftigen sich zum einen mit Fehlklassifikation der Modelle (erste Wahl) und zum Anderen mit der Untersuchung der Beobachtungen, denen das Modell die zweit höchste Wahrscheinlichkeit zugeordnet hat (zweite Wahl). Die zweite Wahl des Modells wird in diesem Abschnitt auch als Nachbarkategorie bezeichnet und wird getrennt betrachtet, je nach dem ob das Modell richtig oder falsch mit der ersten Wahl lag. Wenn folglich von inhaltlich nahen Kategorien gesprochen wird, so ist beispielsweise \textit{weddings} und \textit{divorce} gemeint.\\
\\
Zunächst ist es von Interesse, in welche Kategorien die meisten Beobachtungen klassifiziert werden, wenn nicht die wahre Sparte erkannt wird. Dieser Sachverhalt wird in Abbildung \ref{abb:NeighborMissclassCounts} dargestellt.

\begin{figure}[ht]
    \centering
\includegraphics[width = \textwidth,  keepaspectratio]{Images/NeighborMissclassCounts.pdf} 
\caption{Kategorien mit den meisten Fehlklassifikationen für jede wahre Kategorie für die Modelle der Endauswahl. Die Größe der Punkte zeigt den Anteil der Datenpunkte an, die die Kategorie (Y-Achse) unter allen Fehlklassifikationen annimmt.}
\label{abb:NeighborMissclassCounts}
\end{figure}

Die Datenpunkt in der Abbildung entsprechen der Kategorie, die das Maximum in der \textit{Confusion Matrix} pro Zeile bildet, wenn die Diagonale (\textit{True-Positives}) nicht einbezogen wird. Die Kategorien auf der Y-Achse entsprechen den Kategorien, die das Maximum der \textit{False-Positives} für jede wahre Kategorie annimmt. Die Grafik ist so zu lesen, dass zum Beispiel für die wahre Kategorie \textit{money} bei den Modellen \textit{CNN}, \textit{Bi-LSTM} und \textit{MLP} in die Sparte \textit{business} am häufigsten falsch klassifiziert wird. \textit{XGBoost} prognostiziert am häufigsten die inkorrekte Kategorie \textit{wellness \& healthy living}. Insgesamt ist auffallend, dass größtenteils in die $3$ größten Kategorien \textit{politics}, \textit{wellness \& healthy living} und \textit{entertainment} am häufigsten falsch klassifiziert wird. Besonders für \textit{XGBoost} ist \textit{wellness \& healthy living} meistens die Kategorie mit den meisten Fehlklassifikationen. Dies erklärt auch die hohe \textit{Accuracy} von $0.816$ aus Abbildung \ref{abb:AccByClass}, denn wenn das Modell eine Klasse besonders häufig fehlt, dann ist auch die gesamte Trefferquote für diese Sparte hoch. In einigen Fällen ist die häufigste fehl-klassifizierte Sparte jedoch inhaltlich nahe an der wahren Kategorie. Zum Beispiel ordnet das \textit{CNN} für die wahre Klasse \textit{divorce} die meisten Beobachtungen inkorrekt \textit{weddings} zu. Ebenso wählen \textit{CNN}, \textit{Bi-LSTM} und \textit{MLP} \textit{world news} als häufigste falsche Klasse für \textit{politics}. Das \textit{MLP} wählt \textit{divorce} am häufigsten, wenn \textit{weddings} die richtige Sparte ist. Die Modelle sind sich bei den $11$ Paaren aus wahrer Kategorie und Kategorie mit den meisten Fehlklassifikationen einig. Darunter gehören unter Anderem \textit{black voices} und \textit{entertainment}, \textit{crime} und \textit{politics}, \textit{world news} und \textit{politics}, \textit{wellness \& healthy living} und \textit{parents}. Die wahre Kategorie \textit{wierd news} wird von jedem Modell in eine andere falsche Kategorie eingeordnet. Eine Ursache dafür könnte sein, dass die Schlagzeilen in dieser Sparte inhaltlich sehr verschieden sind. Generell fällt in Abbildung \ref{abb:NeighborMissclassCounts} auf, dass auf der Y-Achse nur $13$ Nachrichtensparten gelistet sind. Daraus ist zu schließen, dass $19$ Kategorien nie als häufigste Kategorie für eine Fehlklassifikation auftreten.\\
\\
Bei dem nächsten Aspekt ist es von Interesse, welche Kategorie am häufigsten die nächst wahrscheinlichste Klasse ist. Zuerst wird dies untersucht für alle Beobachtungen im Testdatensatz, die richtig klassifiziert wurden. Abbildung \ref{abb:NeighborClassesIfTRUE} veranschaulicht diesen Sachverhalt für jede der wahren Kategorien.


\begin{figure}[ht]
    \centering
\includegraphics[width = \textwidth,  keepaspectratio]{Images/NeighborClassesIfTRUE.pdf} 
\caption{Häufigste zweit-wahrscheinlichste Kategorien für die wahren Kategorien aus dem Testdatensatz für die Modelle der Endauswahl. Eingrenzung auf alle Beobachtungen, die korrekt klassifiziert wurden. Die Größe der Punkte zeigt den Anteil der Datenpunkte an, in denen die Kategorie (Y-Achse) die zweite Wahl darstellt.}
\label{abb:NeighborClassesIfTRUE}
\end{figure}

Diese Abbildung basiert im Gegensatz zu Abbildung  \ref{abb:NeighborMissclassCounts} nicht auf der \textit{Confusion Matrix}. Hier wird das Szenario untersucht, dass die Modelle die Kategorie mit der zweit höchsten Wahrscheinlichkeit gewählt hätten. Die Grafik ist so zu lesen, dass zum Beispiel für alle Beobachtungen der Kategorie \textit{money} im Testdatensatz für alle $4$ Modelle \textit{business} die häufigste zweite Wahl ist. \textit{business} ist in diesem Fall eine inhaltlich nahe Nachbarkategorie von \textit{money}.
Es ist wieder erkennbar, dass die $3$ größten Kategorien \textit{politics}, \textit{wellness \& healthy living} und \textit{entertainment} häufig die zweite Wahl sind. Es werden insgesamt $19$ Sparten mindestens einmal als häufigste zweit-wahrscheinlichste Kategorie gewählt, wobei die Modelle sich meistens nicht einig sind. Auffallend ist auch hier, das \textit{XGBoost} in $16$ von $32$ der wahren Kategorien \textit{wellness \& healthy living} am häufigsten die zweitgrößte Wahrscheinlichkeit zuordnet. Paare von inhaltlichen nahen Nachbarkategorien sind in diesem Zusammenhang außerdem \textit{comedy} und \textit{entertainment} (alle Modelle),
\textit{college} und \textit{education} (\textit{CNN}, \textit{MLP}), \textit{divorce} und \textit{weddings} (\textit{Bi-LSTM}, \textit{MLP}), \textit{entertainment} und \textit{comedy} (\textit{Bi-LSTM}), \textit{good news} und \textit{green \& environment} (alle Modelle), \textit{weddings} und \textit{divorce} (\textit{Bi-LSTM}, \textit{MLP}, \textit{CNN}) sowie \textit{world news} und \textit{politics} (alle Modelle). Insgesamt wählen in $9$ Fällen alle Modelle die gleiche Nachbarkategorie am häufigsten. Die Größe der Punkte in der Abbildung indiziert, wie stark die zweit-wahrscheinlichste Kategorie gewählt wird. Bei \textit{comedy} ordnen alle Modelle etwa $80$ Prozent der Beobachtungen \textit{entertainment} als Nachbarkategorie. Bei den wahren Kategorien \textit{entertainment} oder \textit{politics} haben die dominierenden Kategorien als zweite Wahl kleinere Anteile im Bereich $20$-$40$ Prozent. Dies ist ein Indikator dafür, dass die zweit-wahrscheinlichste Klasse unter den Datenpunkten öfter unterschiedlich ausfällt. Zusammenfassend ist zu sehen, dass die häufigste zweite Wahl oft inhaltlich nachvollziehbar an der wahren Klasse liegt. Für \textit{XGBoost} ist nur in einem Fall (\textit{style \& beauty} und \textit{travel}) die häufigste zweite Wahl eine andere als \textit{wellness \& healthy living} oder \textit{politics}.\\
\\

Der letzte zu untersuchende Aspekt ist analog zu dem vorherigen, nur dass diesmal Vorhersagen der Beobachtungen aus dem Testdatensatz betrachtet werden, die sich als falsch erwiesen haben. Es stellt sich hier die Frage, ob im dem Fall der Fehlklassifikation zumindest die häufigste zweit-wahrscheinlichste Klasse korrekt gewesen wäre. Der Sachverhalt ist in Abbildung \ref{abb:NeighborClassesIfFALSE} dargestellt.

\begin{figure}[ht]
    \centering
\includegraphics[width = \textwidth,  keepaspectratio]{Images/NeighborClassesIfFALSE.pdf} 
\caption{Häufigste zweit-wahrscheinlichste Kategorien für die wahren Kategorien aus dem Testdatensatz für die Modelle der Endauswahl. Eingrenzung auf alle Beobachtungen, die inkorrekt klassifiziert wurden. Die Größe der Punkte zeigt den Anteil der Datenpunkte an, in denen die Kategorie (Y-Achse) die zweite Wahl darstellt.}
\label{abb:NeighborClassesIfFALSE}
\end{figure}

Die Punkte auf der Diagonalen der Grafik bedeuten inhaltlich, dass die häufigste zweite Wahl die richtige Kategorie ist. Bis auf wenige Ausnahmen wählen alle Modelle für alle wahren Kategorien die richtige Sparte als zweite Wahl. Die Anteil der Datenpunkte, für die das zutrifft, schwanken zwischen $40$ und $60$ Prozent. \textit{XGBoost} ordnet in $11$ wahren Kategorien die meisten Beobachtungen \textit{politics} als zweit-wahrscheinlichste Klasse zu. Für die wahre Kategorie \textit{college} erkennt nur das \textit{Bi-LSTM} am häufigsten die richte Klasse als zweite Wahl. Dies gilt ebenso für die Sparte \textit{fifty}, wobei hier der Eintrag für das \textit{MLP} fehlt. Der gleiche Eintrag fehlt ebenso in der vorherigen Abbildung \ref{abb:NeighborClassesIfTRUE}. Der Grund dafür ist, dass das \textit{MLP} für keine der Beobachtungen im Testdatensatz die Klasse \textit{fifty} vorhergesagt hat. Dies erklärt auch die $Accuracy$ von $0.00$ in dieser Sparte (siehe Abbildung \ref{abb:AccByClass}). Für \textit{good news} und \textit{money} ist für \textit{XGBoost} \textit{wellness \& healthy living} die häufigste zweite Wahl. Hierbei ist bemerkenswert, dass das \textit{Bi-LSTM} für jede wahre Kategorie die richtige Sparte als zweite Wahl am häufigsten wählen würde. Dies ist ein Merkmal, was zusätzlich zu den Ergebnissen aus Tabelle \ref{tab:finalSelection} für die Qualität dieses Modells spricht.\\
\\
Dieser Abschnitt hat sich mit den korrekten und falschen Klassifikationen auf dem Testdatensatz beschäftigt und das Szenario analysiert, in dem die Modelle die zweit-wahrscheinlichste Klasse wählen. Im nächsten Unterkapitel liegt der Fokus auf der Änderung der Vorhersagen der Modelle, wenn die Daten des Testdatensatzes verändert werden.

\subsection{Veränderung der Vorhersagequalität bei Modifikation der Testdaten}\label{kap:alternate}

Im Folgenden werden die Klassifikationen der Modelle der Endauswahl auf dem Testdatensatz analysiert.
Von den $\numprint{20005}$ Beobachtungen des Testdatensatzes wurden $\numprint{4155}$ Datenpunkte von keinem der $4$ Modelle richtig klassifiziert.
In $\numprint{316}$ Fällen hat jedes Modell eine andere Klasse vorausgesagt. Insgesamt wurden $\numprint{9666}$ Beobachtungen von allen Modellen in die richtige Nachrichtenkategorie eingeordnet. Für diese ab jetzt betrachtete Teilmenge des Testdatensatzes (folgend als $Test_{AllCorrect}$ bezeichnet) erreichen also alle $4$ Modelle eine $Accuracy$ von $1$. \\
Nun ist es von Interesse, wie die Treffsicherheit der Modelle sinkt, wenn die einzelnen Beobachtungen verändert werden. Dabei werden die \textit{Embeddings} \textit{BOW}, \textit{GloVe 300D} und \textit{SOW GloVe 300D} für alle $9666$ mit verschiedenen Methoden modifiziert und anschließend mit den unveränderten gelernten Modellen der Endauswahl neu klassifiziert.\\
\\
Der erste Ansatz $(1)$ ist die Permutation der Wörter den Schlagzeilen. Dabei werden die $maxW = 27$ Spalten der \textit{GloVe 300D} Repräsentation in $30$ Durchläufen einer zufälligen neuen Reihenfolge zugewiesen. Dabei ist anzumerken, dass für jeden Durchlauf die Permutation der Spalten für alle Beobachtungen aus Gründen der Rechenzeit gleich erfolgt. Für jeden Durchlauf wird für die Modellprognosen des \textit{CNN} und \textit{Bi-LSTM} die $Accuracy$ berechnet und anschließend der Mittelwert über alle $Accuracy$ Werte gebildet. 
Diese Kennzahl wird folglich als $\overline{Acc}^{(30)}$ bezeichnet und ist nicht zu verwechseln mit dem Gütemaß $\bar{Accuracy}$, der mittleren Trefferquote über alle Klassen aus Kapitel \ref{kap:guetemass}.
Für die Anzahl der Durchläufe wurde $30$ als heuristischer Wert für alle folgenden Simulationen gewählt. Dieser stellt einen Kompromiss zwischen Minderung der Streuung der mittleren $Accuracy$ und der Rechenzeit der Simulation dar. Diese Simulation der Permutation wird nicht für \textit{BOW} und \textit{SOW GloVe 300D} durchgeführt, da beide \textit{Embeddings} die Reihenfolge der Wörter im Satz nicht berücksichtigen und eine Permutation in keiner Änderung der Vorhersagen resultiert. Die Ergebnisse sind in Tabelle \ref{tab:AlternatePerm} dargestellt.

\begin{table}[ht]
\centering
\begin{tabular}{|l|c|c|}
  \hline
  \textit{Embedding}, Modell & $Accuracy$ vor Perm. & $\overline{Acc}^{(30)}$ nach Perm.\\
  \hline
  \textit{GloVe 300D}, \textit{CNN} & $1$ &  $0.841$ \\
  \textit{GloVe 300D}, \textit{Bi-LSTM} & $1$ & $0.537$ \\
  \hline
\end{tabular}
\caption{$\overline{Acc}^{(30)}$ für die Klassifikationen der $30$ Permutationen von $Test_{AllCorrect}$}
\label{tab:AlternatePerm}

\end{table}

Das \textit{CNN} klassifiziert im Mittel nach Permutation noch $84.1$ Prozent der Beobachtungen in $Test_{AllCorrect}$ korrekt. Dagegen sinkt die mittlere $Accuracy$ für das \textit{Bi-LSTM} auf $0.537$. Hier ist zu sehen, dass die Reihenfolge für das sequentiell gelernte \textit{Bi-LSTM} sehr wichtig ist. Im \textit{CNN} geht nach dem Lernen der \textit{Feature Maps} durch die \textit{Max-Pooling} Operation die Position der gefundenen Wörter-Tupel (-Tripel, -Quadrupel, -Quintupel) verloren. Für das Netz ist das reine Vorkommen wichtig. Selbst nach der Permutation werden teilweise einige Teile der Wörtersequenz noch nebeneinander auftreten, sodass das \textit{CNN} immer noch eine akkurate Vorhersage liefern kann.\\
\\
Im nächsten Ansatz $(2)$ wird untersucht, wie gut die Modelle auf fehlende Wörter reagieren. In jeweils $30$ Durchläufen werden zufällig $1$, $2$, $3$ und $4$ Wörter aus den Schlagzeilen entfernt und anschließend die mittlere $Accuracy$ über die $30$ Modellvorhersagen berechnet. Dabei werden für das \textit{GloVe 300D} \textit{Embedding} jeweils zufällige Spalten mit Nullen gefüllt. Für das \textit{SOW GloVe 300D} wird die gleiche Methodik angewandt, nur dass anschließend noch die normierte Summe gebildet wird. Da es in der \textit{BOW} Repräsentation kein \textit{Padding} gibt und die Spalten für die einzelnen Wörtern an unterschiedlichen Stellen zu finden sind, müssen für jede Beobachtung einzeln die betreffenden Stellen auf $0$ gesetzt werden. Zusätzlich ist anzumerken, dass für den \textit{BOW} die \textit{Stopwords} entfernt wurden. Die meisten Datenpunkte repräsentieren also weniger Wörter als in anderen Repräsentationen der Schlagzeilen, deshalb fällt hier die Entfernung von Wörtern tendenziell etwas stärker ins Gewicht. Die Ergebnisse der Simulation sind in Tabelle \ref{tab:AlternateRem} zusammengefasst.

\begin{table}[ht]
\centering
\begin{tabular}{|l||c|c|c|c|c|}
  \hline
  &  \multicolumn{5}{|c|}{$\overline{Acc}^{(30)}$ nach Entfernung von} \\
  \hline
  \textit{Embedding}, Modell & $0$ Wörtern & $1$ Wort & $2$ Wörtern & $3$ Wörtern & $4$ Wörtern \\
  \hline
   \textit{BOW}, \textit{XGBoost} & $1$ & $0.913$ &  $0.821$ & $0.735$ & $0.658$ \\
   \textit{GloVe 300D}, \textit{CNN} & $1$  & $0.981$ & $0.967$ & $0.953$ & $0.937$ \\
    \textit{GloVe 300D}, \textit{Bi-LSTM} & $1$ & $0.984$ & $0.971$ & $0.958$ & $0.945$ \\
     \textit{SOW GloVe 300D}, \textit{MLP} & $1$  & $0.976$ & $ 0.964$ & $0.953$ & $0.940$\\
     \hline
  
\end{tabular}
\caption{$\overline{Acc}^{(30)}$ für die Klassifikationen der \textit{Embeddings} nach Entfernung von $1,2,3,4$ Wörtern auf $Test_{AllCorrect}$}
\label{tab:AlternateRem}
\end{table}

Bei \textit{XGBoost} sinkt die mittlere Trefferquote am meisten bei Entfernung der Wörter. Dies ist wegen der niedrigeren mittleren Anzahl der Wörter pro Schlagzeile (nach Entfernung der \textit{Stopwords} nicht verwunderlich und der Vergleich der Werte ist mit Vorsicht zu betrachten. Für die restlichen Algorithmen schneidet das \textit{Bi-LSTM} am besten ab, unabhängig davon, wie viele Wörter entfernt wurden. Selbst nach Entfernung von $4$ Wörtern werden im Mittel $94.5$ Prozent der Beobachtungen korrekt von dem \textit{Bi-LSTM} klassifiziert. Die zweit und drittbesten Ergebnisse erzielen das \textit{CNN} und \textit{MLP}, wobei die maximalen Differenzen der $\overline{Acc}^{(30)}$ hier $0.007$ betragen und somit die Unterschiede marginal sind. Sowohl das \textit{CNN} als auch das \textit{Bi-LSTM} reagieren auf die Entfernung von bis zu $4$ Wörtern weit weniger sensibel als auf eine Permutation der Wörter (siehe Tabelle \ref{tab:AlternatePerm}).\\
\\
Im Zuge des letzten Ansatz $(3)$ wird untersucht, wie sich die Trefferquote ändern, wenn nach und nach die Sequenz der Wörter der Schlagzeilen den Modellen zur Verfügung gestellt wird. Dabei werden jeweils alle bis auf die ersten $1, 2, 3, ...$ Wörter aus dem \textit{Embedding} entfernt und anschließend die Vorhersage gebildet. Dies bedeutet für \textit{BOW}, dass analog zu Ansatz 2 die den Wörtern zugehörigen Einträge auf $0$ gesetzt werden. Im Falle des \textit{GloVe 300D} werden die Entsprechenden Wort-Vektoren jeder Beobachtung mit $0$-Vektoren ersetzt. Für das \textit{SOW GloVe 300D} \textit{Embedding} werden anschließend die Summen gebildet. In Abbildung \ref{abb:plotSeqSimulation} sind die durchschnittlichen $Accuracy$'s für die Modelle der Endauswahl im Verlauf der Wörtersequenz dargestellt.

\begin{figure}[ht]
    \centering
\includegraphics[width = \textwidth,  keepaspectratio]{Images/plotSeqSimulation.pdf} 
\caption{Durchschnittliche \textit{Accuracy} der Modelle der Endauswahl bei sequentiellem Input von Teil $1$ bis $15$ der Wörtersequenz}
\label{abb:plotSeqSimulation}
\end{figure}


Die Graphik ist auf den Teil $1$ bis $15$ der Sequenz eingeschränkt worden, da sich im weiteren Verlauf die Reihenfolge der Modelle nicht ändert und die $Accuracy$'s gegen $1$ konvergieren. Wenn die komplette Sequenz als Input dient, dann wird von allen Modellen auf dem unveränderten $Test_{AllCorrect}$ klassifiziert. In der Grafik ist zu erkennen, dass das \textit{MLP} bei Input des ersten Wortes die beste \textit{Accuracy} von allen Modellen erzielt. Bereits nach Teil $2$ der Sequenz wird es von den anderen Modellen eingeholt und befindet sich ab Teil $3$ immer unter den Kurven der anderen Modelle. \textit{XGBoost} erzielt ab Teil $3$ der Sequenz die besten Ergebnisse. Hier ist wieder zu beachten, dass in das \textit{BOW} \textit{Embedding} weniger Wörter pro Beobachtung fließen und deshalb jedes Wort der Sequenz eine höhere Bedeutung hat, als in den anderen \textit{Embeddings}. Das \textit{Bi-LSTM} liefert ab Teil $5$ der Sequenz bessere durchschnittliche $Accuracy$ Werte als das \textit{CNN}, wobei die Unterschiede marginal sind. Bemerkenswert ist, dass bereits nach $8$ Wörtern der Sequenz alle Modelle eine $Accuracy$ von mindestens $0.9$ bei der Klassifikation von erreichen. Durchschnittlich besteht eine Schlagzeile des gesamten Datensatzes aus $11.022$ Wörtern (siehe Kapitel \ref{kap:exploration}).


\subsection{Erklärung der Klassifikation von individuellen Schlagzeilen}

Im letzten Abschnitt fanden die Untersuchungen auf der Teilmenge der Beobachtungen des Testdatensatzes statt, für die alle Modelle die richtige Klasse vorhersagen. Das \textit{Bi-LSTM} auf dem \textit{GloVe 300D} \textit{Embedding} hat sich bezüglich der Gütemaße aus Tabelle \ref{tab:finalSelection} als das performanteste Modell erwiesen. Hier ist es von Interesse, an welchen Stellen dieses Modell zusätzliche Informationen nutzt, um die richtige Klasse zu identifizieren, während die anderen Modelle eine inkorrekte Klasse wählen. Dieser Sachverhalt wird im Folgenden beispielhaft exploriert. Von den $\numprint{20005}$ Schlagzeilen im Testdatensatz wurden $547$ Beobachtungen nur von dem \textit{Bi-LSTM} richtig klassifiziert. Innerhalb dieser Teilmenge gibt es $71$ Datenpunkte, denen zusätzlich von allen Modellen eine andere Kategorie zugeordnet wurde. Im Folgenden werden $2$ dieser $71$ Beobachtungen des Testdatensatzes analysiert. \\
\\
Die erste Schlagzeile lautet: \say{\textit{little girl meets trump impersonator, does not hold back}} und die zugehörige wahre Kategorie ist \textit{comedy}. Die vorhergesagten Modellwahrscheinlichkeiten des Datenpunktes sind in Abbildung \ref{abb:IndProbs13}  für die Nachrichtenkategorien dargestellt.

\begin{figure}[ht]
    \centering
\includegraphics[width = \textwidth,  keepaspectratio]{Images/ggIndProbs13.pdf} 
\caption{Modellwahrscheinlichkeiten der Schlagzeile \textit{little girl meets trump impersonator, does not hold back} für die einzelnen Nachrichtenkategorien}
\label{abb:IndProbs13}
\end{figure}

In der Abbildung ist zu sehen, dass wie gewünscht jedes der $4$ Modelle für eine andere Kategorie die höchste Wahrscheinlichkeit realisiert. Dabei sind die Kategorien, denen von den Modellen hohe Wahrscheinlichkeiten zugeordnet werden, \textit{politics}, \textit{entertainment}, \textit{comedy}, \textit{parents}, \textit{wierd news} und \textit{women}. Müsste mit menschlicher Intuition klassifiziert werden, so scheinen die Sparten \textit{entertainment}, \textit{comedy} und \textit{wierd news} wahrscheinlich und die Kategorien \textit{politics}, \textit{parents} und \textit{women} eher unwahrscheinlich. Nun stellt sich die Frage, wie die Wörter der Schlagzeile sequentiell Einfluss auf die Wahrscheinlichkeiten der einzelnen Kategorien nehmen. Mit Ansatz $(3)$ aus Kapitel \ref{kap:alternate} wird die Schlagzeile nun Wort für Wort den Modellen als \textit{Embedding} zur Verfügung gestellt und die Wahrscheinlichkeiten für die Kategorien berechnet. Abbildung \ref{abb:plotSeqInd13} veranschaulicht dies, wobei aus Gründen der Visualisierung nur die Wahrscheinlichkeiten der $4$ Kategorien gezeichnet sind, die die Modelle voraussagen.

\begin{figure}[!ht]
    \centering
\includegraphics[width = \textwidth,  keepaspectratio]{Images/plotSeqInd13.pdf} 
\caption{Modellwahrscheinlichkeiten der Schlagzeile \textit{little girl meets trump impersonator, does not hold back} für die einzelnen Nachrichtenkategorien bei sequentiellem Input der Wörtersequenz. Die Wörter sind an der Kategorie gekennzeichnet, für die das jeweilige Modell die höchste Wahrscheinlichkeit zum Ende der Sequenz einordnet.}
\label{abb:plotSeqInd13}
\end{figure}

Nach Input der ersten $3$ Wörter \say{\textit{little girl meets}} ordnen alle Modelle \textit{parents} eine hohe Wahrscheinlichkeit zu. Das darauf folgende Wort \say{\textit{trump}} erhöht bei allen Modellen die Zuversicht für \textit{politics}. Zusätzlich wird die Wahrscheinlichkeit für \textit{comedy} und \textit{entertainment} erhöht und die Zuversicht für \textit{parents} stark reduziert. Mit Hinzunahme von \say{\textit{impersonator}} sinkt für alle Modelle außer \textit{XGBoost} die Erwartung für \textit{politics}. Für \textit{XGBoost} ändert sich der Wert nicht und \textit{politics} bleibt bis zum Ende der Sequenz mit einer Wahrscheinlichkeit von $\approx 0.73$ die favorisierte Klasse. Im Falle des \textit{MLP} bewirkt \say{\textit{impersonator}} den Anstieg von \textit{entertainment} zur Klasse mit der höchsten Wahrscheinlichkeit. Das Satzzeichen \say{\textit{,}} wirkt sich minimal auf die Änderung der Wahrscheinlichkeiten aus, doch es senkt die Zuversicht der neuronalen Netze für \textit{comedy} und \textit{entertainment} und erhöht die Zuversicht für \textit{politics}. Im weiteren Verlauf des Satz erhöht sich die Zuversicht des \textit{Bi-LSTM} für \textit{comedy} konstant und nimmt den maximalen Endwert nach \textit{entertainment} an. Für das\textit{CNN} resultiert die gewählte Klasse in \textit{parents}, marginal vor \textit{politics}. Das \textit{MLP} entscheidet sich für \textit{entertainment} mit einer Zuversicht von $\approx 0.4$, wobei es \textit{politics} auch eine moderate Wahrscheinlichkeit zuordnet. Bei Analyse der Abbildung sei angemerkt, dass die Kategorien \textit{queer voices}, \textit{wierd news} und \textit{women} (siehe Abbildung \ref{abb:IndProbs13}) nicht eingezeichnet sind und auch einen Teil der Wahrscheinlichkeit am Ende der Sequenz ausmachen.\\
Die Klassifikation der Schlagzeile ist mit menschlicher Intuition ist nicht eindeutig. In Zusammenhang dazu ist anzumerken, dass sich das \textit{Bi-LSTM} und \textit{CNN} bei diesem Datenpunkt verhältnismäßig unsicher sind und die Zuversicht für die Klassifikation $<  0.25$ beträgt. Im Kontrast dazu ist sich \textit{XGBoost} auf Grund von \say{\textit{trump}} mit $\approx 0.73$ sehr sicher, dass die richtige Klasse \textit{politics} ist, wobei diese Sparte nicht zu den intuitiven zählt. Zusammenfassend zu diesem Beispiel wurde klar, dass sich für die Modelle die Wahl der favorisierte Klasse mit Input der Sequenz öfters ändert. Aufgefallen ist zusätzlich, dass die hinzukommenden Worte die Wahrscheinlichkeiten der Modelle meistens in die gleiche Richtung beeinflussen, wenn auch unterschiedlich stark. So scheinen die Modelle zumindest teilweise ähnliche Informationen für die Worte aus den \textit{Embeddings} zu generieren, auch wenn die Form der \textit{Embeddings} unterschiedlich ist.\\
\\
Das zweite Beispiel ist die Schlagzeile \say{\textit{allen toussaint, legendary pianist, dies at the age of 77}} mit der wahren Kategorie \textit{black voices}. Unter Kenntnis, dass \textit{Allen Toussaint} ein schwarzer amerikanischer Musiker und Songschreiber ist, scheint die Kategorie \textit{black voices} nicht abwegig. Ansonsten könnte die Schlagzeile auch zu \textit{arts \& culture} gehören. Andere Sparten wirken mit menschlicher Intuition eher abwegig. In Abbildung \ref{abb:IndProbs2} sind die Modellwahrscheinlichkeiten des Datenpunktes für die einzelnen Nachrichtensparten dargestellt.

\begin{figure}[ht]
    \centering
\includegraphics[width = \textwidth,  keepaspectratio]{Images/ggIndProbs28.pdf} 
\caption{Modellwahrscheinlichkeiten der Schlagzeile \textit{allen toussaint, legendary pianist, dies at the age of 77} für die einzelnen Nachrichtenkategorien}
\label{abb:IndProbs28}
\end{figure}

Es ist zu sehen, dass den Kategorien \textit{arts \& culture}, \textit{black voices}, \textit{entertainment}, \textit{politics}, \textit{sports} und \textit{wellness \& healthy living} die meiste Wahrscheinlichkeit zugeordnet wurde. Verwunderlich ist vor allem, dass \textit{entertainment} hohe Wahrscheinlichkeiten zugewiesen bekommt, obwohl es sich um eine Todesmeldung handelt. Wie bei dem vorigen Beispiel werden nun die Modellwahrscheinlichkeiten über den sequentiellen Input der Wörter betrachtet. Dies sind in Abbildung \ref{abb:plotSeqInd28} visualisiert.

\begin{figure}[!ht]
    \centering
\includegraphics[width = \textwidth,  keepaspectratio]{Images/plotSeqInd28.pdf} 
\caption{Modellwahrscheinlichkeiten der Schlagzeile \textit{allen toussaint, legendary pianist, dies at the age of 77} für die einzelnen Nachrichtenkategorien bei sequentiellem Input der Wörtersequenz. Die Wörter sind an der Kategorie gekennzeichnet, für die das jeweilige Modell die höchste Wahrscheinlichkeit zum Ende der Sequenz einordnet.}
\label{abb:plotSeqInd28}
\end{figure}

Bei Input der ersten $2$ Wörter \say{\textit{allen toussaint}} erhöht sich bei den neuronalen Netzen die Wahrscheinlichkeit für \textit{black voices}. Der Grund dafür kann sein, dass der Name im Trainingsprozess bereits in der selben Kategorie vorgekommen ist. Bei dem Hinzukommen von \say{\textit{legendary}} steigt die Zuversicht des \textit{Bi-LSTM} und \textbf{MLP} für \textit{black voices} wobei die Zuversicht des \textit{CNN} sinkt. Für alle neuronalen Netze erhöht sich auch die Wahrscheinlichkeit für \textit{entertainment}. \textit{XGBoost} sagt zu Beginn der Sequenz für \textit{wellness \& healthy living} die höchste Wahrscheinlichkeit voraus und ändert die Prognosen im Laufe des Sequenz nur minimal. Es macht den Eindruck, dass \textit{XGBoost} aus der Schlagzeile keine Information für die angezeigten Kategorien extrahieren kann und eine der Klassen mit den meisten Beobachtungen im Trainingsdatensatz prognostiziert. Unter Hinzunahme von \say{\textit{pianist}} erhöht sich die Zuversicht der neuronalen Netze für \textit{arts \& culture}. Für das \textit{CNN} und \textit{MLP} sinkt die Wahrscheinlichkeit für \textit{black voices}, während das \textit{Bi-LSTM} die Wahrscheinlichkeit nicht reduziert. Das nächste \say{\textit{,}} erhöht für das \textit{Bi-LSTM} die Wahrscheinlichkeit der wahren Klasse. Für das \textit{CNN} sinkt ab diesem Teil der Sequenz die Wahrscheinlichkeit für \textit{arts \& culture} leicht ab, doch letztendlich nimmt diese Klasse die höchste Wahrscheinlichkeit von $\approx 0.48$ an. Die Verringerung der Zuversicht für \textit{arts \& culture} passiert auch im Falle des \textit{MLP}, allerdings steigt dort weiterhin die Wahrscheinlichkeit für \textit{entertainment} an, was auch die Sparte ist, die am Ende der Sequenz das Maximum annimmt. Nicht intuitiv ist, dass für das Wort \say{\textit{dies}} bei allen $3$ neuronalen Netzen die Zuversicht für \textit{entertainment} steigt. Schwierig zu Interpretieren ist die starke Schwankung des \textit{Bi-LSTM} bei den Wörtern \say{\textit{age}}, \say{\textit{of}}, \say{\textit{77}} im Vergleich zu den anderen Netzen.



\paragraph{Noch fehlende Punkte:}

\begin{itemize}
    \item todo: vorstellung: struktur der netze, hyperparameter, word embedding sizes und dimensions
    \item tuning wurde klein gehalten, die Modelle werden auf 90 \% der Daten nicht nochmal verändert. (ausser nrounds, epochen, numtrees etc)
\end{itemize}{}



\begin{itemize}
    \item variablenwichtigkeit bei xgboost als wordcloud plotten, dalex und sonstiges
    \item confusionmatrix zeigen
    \item convolutional filters holen und ähnlichkeiten zu combinationen aus word vectors erhalten über tupel/tripel von wortvectoren laufen lassen und schauen wo die ähnlichkeit am größten ist
\end{itemize}{}


\section{Zusammenfassung}

\subsection{Ergebnisse}

\begin{itemize}
    \item Bi-LSTM performt am besten Tabelle \ref{tab:finalSelection}
    \item Bi-LSTM anfällig gegen Permutation, CNN nicht Tabelle \ref{tab:AlternatePerm}
    \item welches Modell ist bzgl wenig rechenzeit und Speichergröße zu empfehlen?
\end{itemize}{}



\subsection{Fazit und Ausblick}

\begin{itemize}
\item unter ergebnisse von Kap 4, die Zusammenlegung der Kategorien nochmal reviewen
    \item Weitere Architekturen im Bereich Bi-LSTM (Attention, sentence level etc)
    \item kompliziertere word embeddings wie encoder technologien wie BERT
    \item meta informationen wie author, text, shortdescriptions hinzuziehen
    \item balancing
    \item alternated Data
    \item alex seine idee
    \item binäre klassifkationen um kategorien zu mergen 
\end{itemize}


\newpage

\end{document}