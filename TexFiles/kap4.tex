\documentclass[a4paper,11pt]{article}

\parindent0cm
\usepackage
[backend=biber,style=apa,sorting=nyt]
{biblatex}
\addbibresource{literature.bib}

\makeatletter
\newcommand\notsotiny{\@setfontsize\notsotiny{8}{8}}
\makeatother

\usepackage{numprint}
\npthousandsep{\,}

\usepackage[utf8]{inputenc}
\usepackage{csquotes}
\usepackage{array}
\usepackage{dirtytalk}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{multirow}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{color}
\usepackage{layouts}
% printing the textsize used
% \printinunitsof{cm}
% \prntlen{\textwidth}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{tabularx}
\usepackage{graphicx}
\usepackage{pdfpages}
\usepackage[ngerman]{babel}
\usepackage[left=3cm,right=2.5cm,top=2cm,bottom=2cm]{geometry}
\renewcommand{\baselinestretch}{1.5}\normalsize % Zeilenabstand 1.5




\begin{document}

\section{Statistische Auswertung}\label{Kap:statAus}

In diesem Kapitel werden die in Kapitel \ref{kap:machineLearning} vorgestellten \textit{Machine Learning} Modelle auf die verschiedenen numerischen Repräsentationen (Kapitel \ref{kap:3.1Wordemb}) der Nachrichten-Schlagzeilen angewandt. Dieses Kapitel ist in $3$ Teile unterteilt. Im ersten Teil wird das Trainings-, Validierungs- und Test- \textit{Framework} beschrieben. Es folgt eine Beschreibung über die Vorauswahl der Modelle. Der zweite Teil behandelt die Evaluation der ausgewählten Modelle anhand der Gütemaße aus Abschnitt \ref{kap:guetemass}. Im dritten Teil werden die Modelle analysiert (todo: vervollständigen)\\
Die Auswertung und Exploration der Daten sowie die Programmierung der Algorithmen erfolgte ausschließlich mit der Statistik-Software \texttt{R} (R Core Team, 2019).

\subsection{Framework und Vorauswahl der Modelle}

Dieses Unterkapitel erläutert das verwendete \textit{Framework} und beschreibt anschließend die stattgefundene Vorauswahl der finalen Modelle, welche in dieser Arbeit verglichen werden.

\subsubsection{Framework}


Dieser Abschnitt erklärt, wie die \numprint{200847} Beobachtungen des gesamten Datensatzes in Trainings-, Test- und Validierungsdaten unterteilen.\\
Die Testdaten ist der Teil der Daten, auf den die finalen Modelle eine Vorhersage der Nachrichtenkategorie treffen und anschließend die Gütemaße aus Kapitel \ref{kap:guetemass} berechnet und evaluiert werden. Wichtig dabei ist, dass die Testdaten den Modellen nie als Input gedient haben. Der Grund dafür ist, dass die Testdaten als Kriterium dafür dienen, wie gut die gelernten Modelle auf ungesehenen Daten generalisieren können. Zur Ermittlung einer geeigneten Größe der Testdaten, wird das Kriterium genommen, dass im Mittel $100$ Beobachtungen in der am niedrigsten repräsentierten Nachrichtensparte enthalten sind. Dies resultiert in einer Stichprobe von $\numprint{20005}$ Beobachtungen, was etwa 10 Prozent der gesamten Daten entspricht. Mit dieser Größe ist eine aussagekräftige Evaluation möglich und gleichzeitig stehen den Algorithmen eine größtmögliche Anzahl von Beobachtungen zum Training zur Verfügung. Von den verbleibenden $90$ der Daten wurde wiederum eine Stichprobe von $10$ Prozent gezogen (insgesamt $9$ Prozent der gesamten Daten). Auf dieser Stichprobe der Größe $\numprint{18084}$ erfolgt die im nächsten Abschnitt beschriebene Vorauswahl der Algorithmen und Repräsentationen. Die Daten für die Vorauswahl (folglich Vorauswahldaten genannt) werden wiederum in $80$ Prozent Trainingsdaten ($\numprint{14467}$ Datenpunkte) und $20$ Prozent  Validierungsdaten ($\numprint{3617}$ Datenpunkte) geteilt. Auf den Vorauswahldaten werden die verschiedenen Repräsentationen der Wörter, sowohl für Training als auch Validierung erstellt. Dabei werden je nach Art des \textit{Embeddings} einige wenige Datenpunkte entfernt. Der Grund hierfür ist beispielsweise, dass  unter Nutzung von \textit{BOW} nach Entfernung der \textit{Stopwords} einige Beobachtungen keine Wörter mehr enthalten. Im Falle von \textit{Glove} werden im Zuge des \textit{paddings} alle Datenpunkte entfernt, die mehr als $W_{max}$ Wörter enthalten.
Letztlich erhalten alle Algorithmen die selben Teilmengen der Vorauswahldaten zum Trainieren und Validieren. Die Struktur der verwendeten neuronalen Netze sowie die Hyperparameter der baumbasierten Verfahren wurde auf den Validierungsdaten für jedes Modell in mehreren Trainings-Durchläufen angepasst.

\subsubsection{Vorauswahl der Modelle}\label{kap:preselection}

Es gibt viele Kombinationen aus Repräsentationen der Wörter und Algorithmen, die auf diese \textit{Embeddings} angewandt werden. In der in diesem Unterabschnitt beschriebenen Vorauswahl wird die Menge der Kombinationen aus \textit{Word Embeddings} und Modellen auf eine Endauswahl reduziert. Die Modelle der Endauswahl werden dann auf den kompletten Trainingsdaten ($\approx 90$ Prozent der Gesamtdaten) trainiert und in Kapitel \ref{kap:evalFinal} auf den Testdaten ($\approx 10$ Prozent der Gesamtdaten) ausgewertet. In die Endauswahl sollen mindestens ein neuronales Netz als auch ein baumbasiertes Verfahren in die Auswahl aufgenommen werden. Des Weiteren ist eine gute Performance im Sinne der Gütemaße aus Kapitel \ref{kap:guetemass} maßgebend. Für die Vorauswahl werden die Gütemaße $accuracy$, $fscore_M^{(1)}$ und $CE$ betrachtet. Für $accuracy$ und $fscore_M^{(1)}$ sind hohe Werte wünschenswert, für $CE$ niedrige Werte.
Die auf den Validierungsdaten evaluierten Ergebnisse sind in Tabelle \ref{tab:preselection} dargestellt.\\


\begin{table}[ht]
\begin{center}
\notsotiny
\begin{tabular}{|m{2cm}||m{1.4cm}m{1.4cm}m{1.4cm}m{1.4cm}m{1.4cm}m{1.4cm}m{1.4cm}|}
\hline
 & \multicolumn{7}{c|}{\textbf{Modelle}}  \\
 \hline
  \textbf{\textit{Word}} \newline \textbf{\textit{Embeddings}} & \textit{XGBoost} & \textit{Random Forest} & \textit{MLP} & \textit{CNN \newline Sequenz} & \textit{CNN GloVe} & \textit{Bi-LSTM \newline Sequenz} &  \textit{Bi-LSTM \newline GloVe} \\
 \hline
 \hline
\textit{BOW} & $0.509$ \newline $0.379$ \newline $1.934$& $0.480$ \newline $0.366$ \newline \textcolor{red}{$5.282$} & $0.504$ \newline $0.301$ \newline $2.066$ & -- & -- & -- & -- \\
\hline
\textit{TFIDF} & $0.496$ \newline $0.374$ \newline $1.998$ & $0.450$ \newline $0.311$ \newline \textcolor{orange}{$4.810$} & $0.510$ \newline $0.331$ \newline $2.070$& -- & -- & -- & -- \\
 \hline
\textit{GloVe unsup. 50D} & -- & -- & -- & -- & $0.382$ \newline $0.195$ \newline $2.379$ & & $0.350$ \newline $0.180$ \newline $2.420$ \\
 \hline
\textit{GloVe 50D} & -- & -- & -- & -- & $0.550$ \newline $0.397$ \newline $1.684$ & -- & $0.537$ \newline $0.408$ \newline $1.680$ \\
 \hline
\textit{GloVe 300D} & -- & -- & -- & -- & \textcolor{LimeGreen}{$0.570$} \newline \textcolor{LimeGreen}{$0.409$} \newline \textcolor{LimeGreen}{$1.637$} & -- & \textcolor{ForestGreen}{$0.587$}  \newline \textcolor{ForestGreen}{$0.467$} \newline \textcolor{ForestGreen}{$1.497$} \\
 \hline
\textit{SOW GloVe unsup. 50D} & \textcolor{red}{$0.273$} \newline \textcolor{orange}{$0.125$} \newline $2.714$ & $0.299$ \newline $0.132$ \newline $4.126$ & \textcolor{orange}{$0.287$} \newline \textcolor{red}{$0.078$} \newline $2.643$ & -- & -- & -- & -- \\
 \hline
\textit{SOW GloVe 50D} & $0.436$ \newline $0.289$ \newline $2.091$ & $0.434$ \newline $0.257$ \newline $4.322$ & $0.418$ \newline $0.187$ \newline $2.100$ & & -- & -- & -- \\
 \hline
\textit{SOW GloVe 300D}  & $0.497$ \newline $0.344$ \newline $1.942$ & $0.447$ \newline $0.249$ \newline $4.210$ & $0.499$ \newline $0.260$ \newline $1.882$ & & -- & -- & -- \\
 \hline
\textit{Sequence} & -- & -- & -- & $0.451$ \newline $0.235$ \newline $2.246$ & -- & $0.477$ \newline $0.263$ \newline $2.194$ & -- \\
   \hline
\end{tabular}

  \caption{Ergebnis von $accuracy$, $fscore_M^{(1)}$ und $CE$ auf den Validierungsdaten für Kombinationen aus \textit{Word Embeddings} und \textit{Machine Learning} Modellen. In Grün ist die Kombination mit der jeweils besten Performance bezüglich eines Maßes markiert. Hellgrün ist die zweitbeste Performance, rot die schlechteste und orange die zweitschlechteste.}  
  \label{tab:preselection}
\end{center}
\end{table}

% \cellcolor[HTML]{AA0044}
Das beste Ergebnis bezüglich allen $3$ Gütemaßen erzielt das \textit{Bi-LSTM} neuronale Netz auf dem \textit{GloVe 300D Embedding} mit einer $accuracy$ von $0.587$, einem $fscore_M^{(1)}$ von $0.467$ und einer $CE$ von $1.497$. Dies bedeutet, dass auf dem Validierungsdatensatz $58.7$ Prozent aller Datenpunkte in der besten Kombination aus \textit{Word Embedding} und \textit{Modell} der richtigen Nachrichtensparte zugeordnet werden kann. Das zweitbeste Modell stellt das \textit{CNN} auf dem selbigen \textit{Embedding} mit einer $accuracy$ von $0.570$, einem $fscore_M^{(1)}$ von $0.409$ und einer $CE$ von $1.637$. Auf den auf Wikipedia basierenden $GloVe 50D$ Wort-Vektoren erzielen beide Modelle etwas schlechtere Ergebnisse, übertreffen aber die Kennzahlen der restlichen Tabelle. Die auf dem Datensatz gelernten \textit{GloVe unsuper. 50D} \textit{Embeddings} erzielen, von den Embeddings die aus Sequenzen von Wort-Vektoren basieren, die schlechtesten Gütemaße. Bei Betrachtung der Summen dieser Vektoren im \textit{SOW GloVe unsup.50D} Embedding fällt auf, dass \textit{XGBoost} bezüglich der $accuracy$ ($0.273$) den niedrigsten und bezüglich dem $fscore_M^{(1)}$ ($0.125$) den zweit niedrigsten Wert erzielt. Unter Nutzung dieser Repräsentation der Wörter nimmt das \textit{MLP} für $fscore_M^{(1)}$ ($0.078$) das schlechteste und für $accuracy$ ($0.287$) das zweit schlechteste Maß an. Im Vergleich der Summen der Sequenzen der Wort-Vektoren, bei denen die Reihenfolge der Wörter nicht mehr einfließt und der Sequenzen der Wort-Vektoren fällt auf, dass letztere Repräsentation, die die Reihenfolge berücksichtigt, in allen Gütemaßen besser abschneidet. Die Repräsentation als Summe erreicht jedoch zumindest bei \textit{SOW GloVe 300D} gute Ergebnisse. Hinsichtlich $accuracy$ ($0.499$) und $CE$ ($1.882$) schneidet \textit{MLP} am besten ab, in Bezug auf den $fscore_M^{(1)}$ ($0.344$) erzielt \textit{XGBoost} den besten Wert auf diesem \textit{Embedding}. Im Bereich des \textit{BOW}, erzielen sowohl \textit{XGBoost}, \textit{Random Forest} und \textit{MLP} solide Ergebnisse. Darunter schneidet XGBoost mit den Werten $0.509$, $0.378$ und $1.934$ am besten ab. Für das $TFIDF$ \textit{Embedding} sind die Ergebnisse bei \textit{XGBoost} und \textit{Random Forest} etwas schlechter und bei \textit{MLP} etwas besser. Die marginalen Unterschiede sind interessant, da in der Berechnung des \textit{TFIDF-Embedding} zusätzliche Aspekte berücksichtigt wurden, zum Beispiel dass selten vorkommende Wörter höher gewichtet und häufig vorkommende Wörter niedriger gewichtet werden. Die beiden mit Abstand höchsten Ausprägungen der \textit{CE} nimmt \textit{Random Forest} mit $5.282$ bei \textit{BOW} und $4.810$ bei \textit{TFIDF} an. Die hohen Werte auf allen \textit{Word Embeddings} sind dadurch zu erklären, dass \textit{Random Forest} kein Verfahren ist, dass die \textit{CE} als Optimierungskriterium nutzt. Die Repräsentation der Wörter, die eine Sequenz aus Wort-Indizes bilden (\textit{Sequence}) erzielen unter der Anwendung von \textit{CNN} ($accuracy = 0.451$) und \textit{Bi-LSTM} ($accuracy = 0.477$) akzeptable Ergebnisse. Auch hier schneidet das \textit{Bi-LSTM} besser als das \textit{CNN} ab. Hierbei werden die Wort-Vektoren mit überwachtem Lernen in einer \textit{Embedding}-Zwischenschicht im neuronalen Netz gelernt. Obwohl das \textit{Sequence} \textit{Embedding} die Reihenfolge der Wörter im Satz berücksichtigt, sind die Ergebnisse schlechter als bei anderen Repräsentationen wie \textit{SOW Glove 300D} oder \textit{BOW}, die die Reihenfolge nicht miteinbeziehen.\\

Zusammenfassend ist in Tabelle \ref{tab:preselection} zu sehen, dass die neuronalen Netze \textit{CNN} und \textit{Bi-LSTM} auf dem \textit{SOW GloVe 300D} die besten Ergebnisse hinsichtlich aller $3$ Gütemaße erzielen. Diese beiden Kombinationen werden deshalb in die Auswahl der finalen Modelle aufgenommen. Auf den klassischen \textit{Embeddings} \textit{BOW} und \textit{TFIDF} erzielen \textit{XGBoost} und \textit{MLP} akzeptable Ergebnisse. Da sich die beiden Repräsentationen hinsichtlich ihrer Struktur wenig unterscheiden, wird ausschließlich ein Modell auf einem der beiden \textit{Embeddings} in die Endauswahl aufgenommen. Dies ist \textit{XGBoost} unter Verwendung von \textit{BOW}, welches für $fscore_M^{(1)}$ und $CE$ das beste und für $accuracy$ das zweitbeste Resultat erzielt. Ein weiteres Modell der Endselektion ist \textit{MLP} auf \textit{SOW GloVe 300D}, welches von allen Repräsentationen, die auf Summen von Wort-Vektoren basieren, die beste \textit{accuracy} und \textit{CE} erzielt. \\

Die vier Kombinationen der Endauswahl umfassen nun $4$ Modelle und $3$ Arten der Repräsentationen der Wörter und werden in den nächsten Abschnitten näher vorgestellt.




\begin{itemize}
    \item tuning wurde klein gehalten, die Modelle werden auf 90 \% der Daten nicht nochmal verändert. (ausser nrounds, epochen, numtrees etc)
\end{itemize}{}

\subsection{Evaluation und Vergleich der Modelle} \label{kap:evalFinal}
\subsubsection{Performanzmaße}

\begin{itemize}
\item vorstellung: struktur, hyperparameter, word embedding sizes und dimensions
    \item metriken wie in vorauswahl nur ausführlicher
    \item accuracy by class comparison
    \item confidence vs accuracy plots
    \item maß wie sicher ist sich das Verfahren, wenn es die richtige klasse ist? compareProbVsAcc/CE
\end{itemize}{}

\subsection{Analyse der Modelle}

\begin{itemize}

    \item beobachtungen verändern, wörter wegnehmen, hinzufügen, reihenfolge ändern und schauen ob das verfahren stabil /sensitiv zur reihenfolge
    \item variablenwichtigkeit bei xgboost als wordcloud plotten, dalex und sonstiges
    \item random forest direkt variablenwichtigkeit
    \item nachbarklassen identifizieren durch confusionmatrix
    \item confusionmatrix zeigen
    \item convolutional filters holen und ähnlichkeiten zu combinationen aus word vectors erhalten über tupel/tripel von wortvectoren laufen lassen und schauen wo die ähnlichkeit am größten ist
\end{itemize}{}

\subsubsection{Anpassung des besten Modell auf den gesamten Datensatz}



\end{document}