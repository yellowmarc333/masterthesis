\documentclass[a4paper,11pt]{article}

\parindent0cm
\usepackage
[backend=biber,style=apa,sorting=nyt]
{biblatex}
\addbibresource{literature.bib}

\makeatletter
\newcommand\notsotiny{\@setfontsize\notsotiny{8}{8}}
\newcommand\micro{\@setfontsize\notsotiny{4.5}{4.5}}
\newcommand\middletiny{\@setfontsize\notsotiny{6}{6}}
\makeatother

\usepackage{numprint}
\npthousandsep{\,}

\usepackage[utf8]{inputenc}
\usepackage{csquotes}
\usepackage{array}
\usepackage{dirtytalk}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{multirow}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{color}
\usepackage{layouts}
% printing the textsize used
% \printinunitsof{cm}
% \prntlen{\textwidth}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{tabularx}
\usepackage{graphicx}
\usepackage{pdfpages}
\usepackage[ngerman]{babel}
\usepackage[left=3cm,right=2.5cm,top=2cm,bottom=2cm]{geometry}
\renewcommand{\baselinestretch}{1.5}\normalsize % Zeilenabstand 1.5




\begin{document}

\section{Statistische Auswertung}\label{Kap:statAus}

In diesem Kapitel werden die in Kapitel \ref{kap:machineLearning} vorgestellten \textit{Machine Learning} Modelle auf die verschiedenen numerischen Repräsentationen (Kapitel \ref{kap:3.1Wordemb}) der Nachrichten-Schlagzeilen angewandt. Dieses Kapitel ist in $3$ Teile unterteilt. Im ersten Teil wird das Trainings-, Validierungs- und Test-\textit{Framework} beschrieben. Es folgt eine Beschreibung über die Vorauswahl der Modelle. Der zweite Teil behandelt die Evaluation der ausgewählten Modelle anhand der Gütemaße aus Abschnitt \ref{kap:guetemass}. Im dritten Teil werden die Modelle analysiert (todo: vervollständigen)\\
Die Auswertung und Exploration der Daten sowie die Programmierung der Algorithmen erfolgte ausschließlich mit der Statistik-Software \texttt{R} (R Core Team, 2019).

\subsection{Framework und Vorauswahl der Modelle}

Dieses Unterkapitel erläutert das verwendete \textit{Framework} und beschreibt anschließend die stattgefundene Vorauswahl der finalen Modelle, welche in dieser Arbeit verglichen werden.

\subsubsection{Framework}


Dieser Abschnitt erklärt, wie die \numprint{200847} Beobachtungen des gesamten Datensatzes in \\
Trainings-,Test- und Validierungsdaten unterteilt werden.
Die Testdaten sind der Teil der Daten, auf den die finalen Modelle eine Vorhersage der Nachrichtenkategorie treffen und anschließend die Gütemaße aus Kapitel \ref{kap:guetemass} berechnet und evaluiert werden. Wichtig dabei ist, dass die Testdaten den Modellen nie als Input gedient haben. Der Grund dafür ist, dass die Testdaten als Kriterium dafür dienen, wie gut die gelernten Modelle auf ungesehenen Daten generalisieren können. Zur Ermittlung einer geeigneten Größe der Testdaten wird das Kriterium genommen, dass im Mittel $100$ Beobachtungen in der am niedrigsten repräsentierten Nachrichtensparte enthalten sind. Dies resultiert in einer Stichprobe von $\numprint{20005}$ Beobachtungen, was etwa 10 Prozent der gesamten Daten entspricht. 
Mit dieser Größe ist eine aussagekräftige Evaluation möglich und gleichzeitig stehen den Algorithmen eine größtmögliche Anzahl von Beobachtungen zum Training zur Verfügung. Von den verbleibenden $90$ Prozent ($\numprint{180842}$ Datenpunkte) der Daten wurde wiederum eine Stichprobe von $10$ Prozent gezogen (insgesamt $9$ Prozent der gesamten Daten). Auf dieser Stichprobe der Größe $\numprint{18084}$ erfolgt die im nächsten Abschnitt beschriebene Vorauswahl der Algorithmen und Repräsentationen. Die Daten für die Vorauswahl (folglich Vorauswahldaten genannt) werden wiederum in $80$ Prozent Trainingsdaten ($\numprint{14467}$ Datenpunkte) und $20$ Prozent  Validierungsdaten ($\numprint{3617}$ Datenpunkte) geteilt. Auf den Vorauswahldaten werden die verschiedenen Repräsentationen der Wörter, sowohl für Training als auch Validierung erstellt. Dabei werden je nach Art des \textit{Embeddings} einige wenige Datenpunkte entfernt. Der Grund hierfür ist beispielsweise, dass  unter Nutzung von \textit{BOW} nach Entfernung der \textit{Stopwords} einige Beobachtungen keine Wörter mehr enthalten. Im Falle von \textit{Glove} werden im Zuge des \textit{paddings} alle Datenpunkte entfernt, die mehr als $W_{max}$ Wörter enthalten.
Letztlich erhalten alle Algorithmen die selben Teilmengen der Vorauswahldaten zum Trainieren und Validieren. Die Struktur der verwendeten neuronalen Netze sowie die Hyperparameter der Verfahren wurde auf den Validierungsdaten für jedes Modell in mehreren Trainings-Durchläufen angepasst.

\subsubsection{Vorauswahl der Modelle}\label{kap:preselection}

Es gibt viele Kombinationen aus Repräsentationen der Wörter und Algorithmen, die auf diese \textit{Embeddings} angewandt werden. In der in diesem Unterabschnitt beschriebenen Vorauswahl wird die Menge der Kombinationen aus \textit{Word Embeddings} und Modellen auf eine Endauswahl reduziert. Die Modelle der Endauswahl werden dann auf den kompletten Trainingsdaten ($\approx 90$ Prozent der Gesamtdaten) trainiert und in Kapitel \ref{kap:evalFinal} auf den Testdaten ($\approx 10$ Prozent der Gesamtdaten) ausgewertet. In die Endauswahl sollen mindestens ein neuronales Netz als auch ein baumbasiertes Verfahren in die Auswahl aufgenommen werden. Des Weiteren ist eine gute Performance im Sinne der Gütemaße aus Kapitel \ref{kap:guetemass} maßgebend. Für die Vorauswahl werden die Gütemaße $accuracy$, $fscore_M^{(1)}$ und $CE$ betrachtet. Für $accuracy$ und $fscore_M^{(1)}$ sind hohe Werte wünschenswert, für $CE$ niedrige Werte.
Die auf den Validierungsdaten evaluierten Ergebnisse sind in Tabelle \ref{tab:preselection} dargestellt.\\


\begin{table}[ht]
\begin{center}
\notsotiny
\begin{tabular}{|m{2cm}||m{1.4cm}m{1.4cm}m{1.4cm}m{1.4cm}m{1.4cm}m{1.4cm}m{1.4cm}|}
\hline
 & \multicolumn{7}{c|}{\textbf{Modelle}}  \\
 \hline
  \textbf{\textit{Word}} \newline \textbf{\textit{Embeddings}} & \textit{XGBoost} & \textit{Random Forest} & \textit{MLP} & \textit{CNN \newline Sequenz} & \textit{CNN GloVe} & \textit{Bi-LSTM \newline Sequenz} &  \textit{Bi-LSTM \newline GloVe} \\
 \hline
 \hline
\textit{BOW} & $0.509$ \newline $0.379$ \newline $1.934$& $0.480$ \newline $0.366$ \newline \textcolor{red}{$5.282$} & $0.504$ \newline $0.301$ \newline $2.066$ & -- & -- & -- & -- \\
\hline
\textit{TFIDF} & $0.496$ \newline $0.374$ \newline $1.998$ & $0.450$ \newline $0.311$ \newline \textcolor{orange}{$4.810$} & $0.510$ \newline $0.331$ \newline $2.070$& -- & -- & -- & -- \\
 \hline
\textit{GloVe unsup. 50D} & -- & -- & -- & -- & $0.382$ \newline $0.195$ \newline $2.379$ & & $0.350$ \newline $0.180$ \newline $2.420$ \\
 \hline
\textit{GloVe 50D} & -- & -- & -- & -- & $0.550$ \newline $0.397$ \newline $1.684$ & -- & $0.537$ \newline $0.408$ \newline $1.680$ \\
 \hline
\textit{GloVe 300D} & -- & -- & -- & -- & \textcolor{LimeGreen}{$0.570$} \newline \textcolor{LimeGreen}{$0.409$} \newline \textcolor{LimeGreen}{$1.637$} & -- & \textcolor{ForestGreen}{$0.587$}  \newline \textcolor{ForestGreen}{$0.467$} \newline \textcolor{ForestGreen}{$1.497$} \\
 \hline
\textit{SOW GloVe unsup. 50D} & \textcolor{red}{$0.273$} \newline \textcolor{orange}{$0.125$} \newline $2.714$ & $0.299$ \newline $0.132$ \newline $4.126$ & \textcolor{orange}{$0.287$} \newline \textcolor{red}{$0.078$} \newline $2.643$ & -- & -- & -- & -- \\
 \hline
\textit{SOW GloVe 50D} & $0.436$ \newline $0.289$ \newline $2.091$ & $0.434$ \newline $0.257$ \newline $4.322$ & $0.418$ \newline $0.187$ \newline $2.100$ & & -- & -- & -- \\
 \hline
\textit{SOW GloVe 300D}  & $0.497$ \newline $0.344$ \newline $1.942$ & $0.447$ \newline $0.249$ \newline $4.210$ & $0.499$ \newline $0.260$ \newline $1.882$ & & -- & -- & -- \\
 \hline
\textit{Sequence} & -- & -- & -- & $0.451$ \newline $0.235$ \newline $2.246$ & -- & $0.477$ \newline $0.263$ \newline $2.194$ & -- \\
   \hline
\end{tabular}

  \caption{Ergebnis von $accuracy$, $fscore_M^{(1)}$ und $CE$ auf den Validierungsdaten für Kombinationen aus \textit{Word Embeddings} und \textit{Machine Learning} Modellen. In grün ist die Kombination mit der jeweils besten Performance bezüglich eines Maßes markiert. Hellgrün ist die zweitbeste Performance, rot die schlechteste und orange die zweitschlechteste.}  
  \label{tab:preselection}
\end{center}
\end{table}

% \cellcolor[HTML]{AA0044}
Das beste Ergebnis bezüglich allen $3$ Gütemaßen erzielt das \textit{Bi-LSTM} neuronale Netz auf dem \textit{GloVe 300D Embedding} mit einer $accuracy$ von $0.587$, einem $fscore_M^{(1)}$ von $0.467$ und einer $CE$ von $1.497$. Dies bedeutet, dass auf dem Validierungsdatensatz $58.7$ Prozent aller Datenpunkte in der besten Kombination aus \textit{Word Embedding} und \textit{Modell} der richtigen Nachrichtensparte zugeordnet werden kann. Das zweitbeste Modell stellt das \textit{CNN} auf dem selbigen \textit{Embedding} mit einer $accuracy$ von $0.570$, einem $fscore_M^{(1)}$ von $0.409$ und einer $CE$ von $1.637$. Auf den auf Wikipedia basierenden $GloVe 50D$ Wort-Vektoren erzielen beide Modelle etwas schlechtere Ergebnisse, übertreffen aber die Kennzahlen der restlichen Tabelle. Die auf dem Datensatz gelernten \textit{GloVe unsuper. 50D} \textit{Embeddings} erzielen, von den Embeddings die aus Sequenzen von Wort-Vektoren basieren, die schlechtesten Gütemaße. Bei Betrachtung der Summen dieser Vektoren im \textit{SOW GloVe unsup.50D} Embedding fällt auf, dass \textit{XGBoost} bezüglich der $accuracy$ ($0.273$) den niedrigsten und bezüglich dem $fscore_M^{(1)}$ ($0.125$) den zweit niedrigsten Wert erzielt. Unter Nutzung dieser Repräsentation der Wörter nimmt das \textit{MLP} für $fscore_M^{(1)}$ ($0.078$) das schlechteste und für $accuracy$ ($0.287$) das zweit schlechteste Maß an. Im Vergleich der Summen der Sequenzen der Wort-Vektoren, bei denen die Reihenfolge der Wörter nicht mehr einfließt und der Sequenzen der Wort-Vektoren fällt auf, dass letztere Repräsentation, die die Reihenfolge berücksichtigt, in allen Gütemaßen besser abschneidet. Die Repräsentation als Summe erreicht jedoch zumindest bei \textit{SOW GloVe 300D} gute Ergebnisse. Hinsichtlich $accuracy$ ($0.499$) und $CE$ ($1.882$) schneidet \textit{MLP} am besten ab, in Bezug auf den $fscore_M^{(1)}$ ($0.344$) erzielt \textit{XGBoost} den besten Wert auf diesem \textit{Embedding}. Im Bereich des \textit{BOW}, erzielen sowohl \textit{XGBoost}, \textit{Random Forest} und \textit{MLP} solide Ergebnisse. Darunter schneidet XGBoost mit den Werten $0.509$, $0.378$ und $1.934$ am besten ab. Für das $TFIDF$ \textit{Embedding} sind die Ergebnisse bei \textit{XGBoost} und \textit{Random Forest} etwas schlechter und bei \textit{MLP} etwas besser. Die marginalen Unterschiede sind interessant, da in der Berechnung des \textit{TFIDF-Embedding} zusätzliche Aspekte berücksichtigt wurden, zum Beispiel dass selten vorkommende Wörter höher gewichtet und häufig vorkommende Wörter niedriger gewichtet werden. Die beiden mit Abstand höchsten Ausprägungen der \textit{CE} nimmt \textit{Random Forest} mit $5.282$ bei \textit{BOW} und $4.810$ bei \textit{TFIDF} an. Die hohen Werte auf allen \textit{Word Embeddings} sind dadurch zu erklären, dass \textit{Random Forest} kein Verfahren ist, dass die \textit{CE} als Optimierungskriterium nutzt. Die Repräsentation der Wörter, die eine Sequenz aus Wort-Indizes bilden (\textit{Sequence}) erzielen unter der Anwendung von \textit{CNN} ($accuracy = 0.451$) und \textit{Bi-LSTM} ($accuracy = 0.477$) akzeptable Ergebnisse. Auch hier schneidet das \textit{Bi-LSTM} besser als das \textit{CNN} ab. Hierbei werden die Wort-Vektoren mit überwachtem Lernen in einer \textit{Embedding}-Zwischenschicht im neuronalen Netz gelernt. Obwohl das \textit{Sequence} \textit{Embedding} die Reihenfolge der Wörter im Satz berücksichtigt, sind die Ergebnisse schlechter als bei anderen Repräsentationen wie \textit{SOW Glove 300D} oder \textit{BOW}, die die Reihenfolge nicht miteinbeziehen.\\

Zusammenfassend ist in Tabelle \ref{tab:preselection} zu sehen, dass die neuronalen Netze \textit{CNN} und \textit{Bi-LSTM} auf dem \textit{SOW GloVe 300D} die besten Ergebnisse hinsichtlich aller $3$ Gütemaße erzielen. Diese beiden Kombinationen werden deshalb in die Auswahl der finalen Modelle aufgenommen. Auf den klassischen \textit{Embeddings} \textit{BOW} und \textit{TFIDF} erzielen \textit{XGBoost} und \textit{MLP} akzeptable Ergebnisse. Da sich die beiden Repräsentationen hinsichtlich ihrer Struktur wenig unterscheiden, wird ausschließlich ein Modell auf einem der beiden \textit{Embeddings} in die Endauswahl aufgenommen. Dies ist \textit{XGBoost} unter Verwendung von \textit{BOW}, welches für $fscore_M^{(1)}$ und $CE$ das beste und für $accuracy$ das zweitbeste Resultat erzielt. Ein weiteres Modell der Endselektion ist \textit{MLP} auf \textit{SOW GloVe 300D}, welches von allen Repräsentationen, die auf Summen von Wort-Vektoren basieren, die beste \textit{accuracy} und \textit{CE} erzielt. \\

Die vier Kombinationen der Endauswahl umfassen nun $4$ Modelle und $3$ Arten der Repräsentationen der Wörter und werden in den nächsten Abschnitten näher vorgestellt.




\begin{itemize}
    \item tuning wurde klein gehalten, die Modelle werden auf 90 \% der Daten nicht nochmal verändert. (ausser nrounds, epochen, numtrees etc)
\end{itemize}{}

\subsection{Evaluation und Vergleich der Modelle} \label{kap:evalFinal}

In diesem Abschnitt werden nun die finalen Modelle auf den etwa $90$ Prozent der Gesamtdaten ($180842$ Datenpunkte) trainiert und auf den etwa $10$ Prozent Testdaten ($20005$ Datenpunkte) anhand der resultierenden Modellwahrscheinlichkeiten und den \textit{Confusion Matrizes} evaluiert. Die Ergebnisse sind in Tabelle \ref{tab:finalSelection} zusammengetragen. 


\begin{table}[ht]
\centering
\begin{tabular}{|l||ccccc|}
  \hline
\textit{Embedding}, Modell, (Nummer) & $accuracy$ & $\overline{accuracy}$ & $fscore_M^{(1)}$ & $CE$ & $\bar{p}_{IfCorrect}$ \\ 
  \hline
\textit{BOW, XGBoost}, (1) & \textcolor{red}{0.624} & 0.487 & 0.541 & \textcolor{red}{1.447} & 0.653 \\ 
  \textit{GloVe 300D, CNN}, (2) & 0.668 & 0.521 & 0.562 & 1.247 & \textcolor{red}{0.646} \\ 
  \textit{GloVe 300D, Bi-LSTM}, (3) & \textcolor{ForestGreen}{0.689} & \textcolor{ForestGreen}{0.552} & \textcolor{ForestGreen}{0.587} & \textcolor{ForestGreen}{1.124} & \textcolor{ForestGreen}{0.825} \\ 
  \textit{SOW GloVe 300D, MLP}, (4) & 0.625 & \textcolor{red}{0.455} & \textcolor{red}{0.499} & 1.358 & 0.705 \\ 
   \hline
\end{tabular}
\caption{Evaluation der Modelle der finalen Auswahl bezüglich der Gütemaße $accuracy$, $\overline{accuracy}$, $fscore_M^{(1)}$, $CE$ und  $\bar{p}_{IfCorrect}$. In dunkelgrün ist das beste Ergebnis pro Gütemaß markiert, in rot das schlechteste}
\label{tab:finalSelection}

\end{table}

Zur Beschreibung aller folgenden Ergebnisse wird aus Gründen der Platzersparnis nur noch der Name des Algorithmus ohne das zugehörige \textit{Word-Embedding} genannt.
In der Tabelle \ref{tab:finalSelection} ist zu sehen, dass das \textit{Bi-LSTM} bezüglich allen betrachteten Gütemaßen am besten abschneidet. Mit einer \textit{accuracy} von $0.689$ sagt es für die Beobachtungen des Testdatensatzes in $68.9$ Prozent der Fälle die richtige der $32$ Klassen vorher. Werden kleinere Klassen wichtiger angesehen, so erkennt das \textit{Bi-LSTM} im Mittel $55.2$ Beobachtungen pro Sparte und erzielt im Sinne des $fscore_M^{(1)}$ einen Wert von $0.582$. Bezüglich der $CE$, die auch als Verlustfunktion im Trainingsprozess minimiert wird, wird ein Wert von $1.124$ erreicht. Wenn das Modell die richtige Klasse vorhersagt, so ist es sich mit einer mittleren Modellwahrscheinlichkeit von $0.825$ mit Abstand am sichersten. Das zweitbeste Modell ist das \textit{CNN} in Bezug auf \textit{accuracy} ($0.668$),  $\overline{accuracy}$ ($0.521$), $fscore_M^{(1)}$ ($0.562$) und $CE$ ($1.247$). Im Falle einer richtigen Klassifikation ist sich dieses Modell jedoch am unsichersten ($\bar{p}_{IfCorrect} =0.646$). Den dritten Platz in Hinsicht auf $accuracy$ belegt das \textit{MLP} ($0.625$), dicht gefolgt von $XGBoost$ ($0.624$). Bei den beiden Gütemaßen, bei denen kleinere Klassen eine höhere Wichtigkeit besitzen, schneidet das \textit{MLP} am schlechtesten ab ($\overline{accuracy} = 0.455$, $fscore_M^{(1)} = 0.499$). \textit{XGBoost} belegt bezüglich der $CE$ den letzten Platz.\\
Nachdem nun die globale Performance auf dem kompletten Testdatensatz untersucht wurde, erfolgt nun eine Betrachtung für die einzelnen Nachrichtensparten. Abbildung \ref{abb:AccByClass} zeigt die $accuracy$ pro Klasse für alle Modelle der Endauswahl.

\begin{figure}[ht]
    \centering
\includegraphics[width = \textwidth,  keepaspectratio]{Images/FinalSelectionAccByClass.pdf} 
\caption{\textit{accuracy} je Kategorie für die $4$ Modelle der Endauswahl. Geordnet absteigend von links nach rechts nach Größe der Kategorie im Testdatensatz}
\label{abb:AccByClass}
\end{figure}

Die Grafik ist so zu betrachten, dass die Sparten nach der Anzahl Beobachtungen absteigend von links nach rechts geordnet sind. Es ist sichtbar, dass \textit{Bi-LSTM} in $22$ der $32$ Kategorien die beste $accuracy$ annimmt und gleichzeitig in keiner Sparte am schlechtesten performt. Das \textit{CNN} belegt in $8$ Kategorien den ersten Platz, unter anderem für die beiden kleinsten Klassen \textit{college} und \textit{education}. Nur in \textit{impact} erreicht das \textit{CNN} die niedrigste Trefferrate.
\textit{XGBoost} schneidet in $12$ Kategorien am schlechtesten ab und in keiner am besten. Das \textit{MLP} performt am besten in der Kategorie \textit{sports}, belegt aber in $16$ der $32$ Sparten den letzten Platz. Besonders schlecht schneidet dieses Modell bei den $5$ kleinsten Kategorien ab. In der Sparte \textit{fifty} erreicht das \textit{MLP} als einziges Modell in einer Kategorie eine Trefferquote von $0$ Prozent. Der Bezug zu den Maßen $\overline{accuracy}$ ($0.455$) und $fscore_M^{(1)}$ ($0.499)$ für das \textit{MLP} aus Tabelle \ref{tab:finalSelection} kann hergestellt werden, da das \textit{MLP} in vielen Kategorien schlecht und nur in wenigen gut abschneidet. \\
Generell ist in Abbildung \ref{abb:AccByClass} zu beobachten, dass die $2$ größten Kategorien \textit{politics} und \textit{wellness \& healthy living} sowie die Sparten \textit{divorce} und \textit{tech} etwa gleich gut von allen Modellen vorhergesagt werden. Kleine Kategorien schneiden tendenziell schlechter ab, was in den generell niedrigeren Maßen $\overline{accuracy}$ und $fscore_M^{(1)}$ in Tabelle \ref{tab:finalSelection} resultiert. Die $7$ Kategorien mit den meisten Beobachtungen erzielen unter Verwendung des jeweils besten Modells eine $accuracy$ zwischen $0.76$ und $0.86$ Prozent. \\
\\
Nachfolgend wird untersucht, wie hoch die Modelle die Wahrscheinlichkeit für die richtige Klasse einschätzen und ob diese Wahrscheinlichkeit proportional zu der Anzahl der korrekt klassifizierten Beobachtungen ist. Dazu werden $50$ (heuristischer Wert) Intervalle $([0, 0.02), [0.02, 0.04), \dots, [0.98, 1]$ gebildet und für jede der $19999$ Beobachtungen im Testdatensatz die Wahrscheinlichkeit des Modells für die richtige Klasse erfasst. Gleichzeitig wird in einer logischen Variable (nimmt $1$ für korrekt und $0$ für inkorrekt an) festgehalten, ob das Modell richtig gelegen hat, was gleichbedeutend dazu ist, dass das Modell keiner anderen Klasse eine höhere Wahrscheinlichkeit zugeordnet hat.
Es erfolgt anschließend die Zuordnung Wahrscheinlichkeiten zu den entsprechenden Intervallen. Für alle Intervalle wird nun der Mittelwert über die binäre Variable gebildet. Dieser Mittelwert ist der Anteil der korrekt klassifizierten Beobachtungen pro Intervall der Modellwahrscheinlichkeit und ist für alle Modelle der Endauswahl in Abbildung \ref{abb:CompareProbVsAcc} dargestellt.


\begin{figure}[ht]
    \centering
\includegraphics[width = \textwidth,  keepaspectratio]{Images/FinalSelectionCompareProbVsAcc.pdf} 
\caption{Anteil korrekt klassifizierter Beobachtungen für Intervalle der Modellwahrscheinlichkeiten der $4$ Modelle der Endauswahl. Die gestrichelte Linie markiert das Verhältnis 1:1}
\label{abb:CompareProbVsAcc}
\end{figure}

Wie die Grafik zu lesen ist sei an einem Beispiel verdeutlicht: Für die Mitte des Intervals der Modellwahrscheinlichkeit von $0.75$ (dies sind alle Beobachtungen die eine Wahrscheinlichkeit für die richtige Klasse in $(0.74,0.76]$ von dem Modell zugeordnet bekommen haben) liegt im Falle des \textit{Bi-LSTM} der Anteil der korrekt klassifizierten Beobachtungen bei etwa $0.66$. In diesem Fall ist sich das Modell also sicherer, die korrekte Klasse vorherzusagen, als es letztendlich nach Evaluation der Fall ist. 
Da sich der Verlauf für das \textit{Bi-LSTM} größtenteils unter der gestrichelten Linie abzeichnet, spricht dies dafür, dass dieses Modell die Wahrscheinlichkeit der richtigen Klasse überschätzt. Dies kann auch den hohen Wert von $\bar{p}_{IfCorrect}$ ($0.825$) aus Tabelle \ref{tab:finalSelection} erklären; wenn das Modell generell hohe Wahrscheinlichkeiten für die wahre Klassen zuteilt, ist der Mittelwert für alle korrekten Einschätzungen (eine Teilmenge davon) auch hoch. Sowohl bei \textit{XGBoost} als auch bei dem \textit{CNN} tritt der gegenteilige Effekt des Unterschätzens der Wahrscheinlichkeit für die richtige Klasse auf, wobei besonders das \textit{CNN} für die hohen Wahrscheinlichkeiten eine noch höhere Trefferquote besitzt. Das \textit{MLP} bewegt sich nahe an der gestrichelten Linie und ist sich tendenziell genau so sicher, wie es auch korrekt klassifiziert. Die Schwankungen sind im linken Teil der Graphik für alle Modelle höher. Dies ist dadurch zu erklären, dass Modelle mit einer guten Performance seltener niedrige Wahrscheinlichkeiten für die richtige Klasse modellieren. Demnach ist die Anzahl der Beobachtungen für kleine Intervall-Mitten geringer und resultiert in einer höheren Schwankung des Anteils der korrekt klassifizierten Beobachtungen. Aus dem gleichen Grund fehlen auch einige Punkte, da für diese keine Beobachtungen im Intervall zu finden sind. \\
\\
In diesem Unterkapitel wurden die Gütemaße für die Modelle der Endauswahl erläutert und die Sicherheit der Modelle untersucht.
Im nächsten Abschnitt beschäftigt sich mit der Untersuchung von benachbarten Kategorien. todo: bessere Überleitung finden

\subsection{Nachbarkategorien}

In diesem Abschnitt werden $2$ Aspekte behandelt. Es stellt sich zum einen für jede Kategorie die Frage, welche, im Falle einer Fehlklassifikation des Modells, die Sparte ist, der die meisten Beobachtungen zugeordnet werden. Zum Anderen ist es von Interesse, in welche Kategorien, im Falle einer korrekten Klassifikation, die nächst wahrscheinlichste Klasse gewesen wäre.


\begin{itemize}
    \item zweite Wahl ist wahrscheinlichkeitsbasiert
\end{itemize}{}



\begin{table}[ht]
\notsotiny
\centering
\begin{tabular}{|l||rrrr|}
  \hline
  & \multicolumn{4}{c|}{\textbf{Nachbarkategorie, (Beobachtungen)}} \\
  \hline
Wahre Kategorie & \textit{BOW}, \textit{XGBoost} & \textit{GloVe 300D}, \textit{CNN} & \textit{GloVe 300D}, \textit{Bi-LSTM} & \textit{SOW GloVe 300D}, \textit{MLP} \\ 
  \hline
\textit{arts \& culture} & \textit{wellness \& \dots}, ($62$) & \textit{entertainment}, ($57$) & \textit{entertainment}, ($43$) & \textit{entertainment}, ($64$) \\ 
  \textit{black voices} & \textit{entertainment}, ($67$) & \textit{entertainment}, ($88$) & \textit{entertainment}, ($62$) & \textit{entertainment}, ($88$) \\ 
  \textit{business} & \textit{wellness \& \dots}, ($141$) & \textit{wellness \& \dots}, ($89$) & \textit{wellness \& \dots}, ($85$) & \textit{politics}, ($109$) \\ 
  \textit{college} & \textit{wellness \& \dots}, ($19$) & \textit{wellness \& \dots}, ($13$) & \textit{politics}, ($11$) & \textit{politics}, ($18$) \\ 
  \textit{comedy} & \textit{wellness \& \dots}, ($78$) & \textit{entertainment}, ($83$) & \textit{entertainment}, ($79$) & \textit{politics}, ($101$) \\ 
  \textit{crime} & \textit{politics}, ($41$) & \textit{politics}, ($50$) & \textit{politics}, ($37$) & \textit{politics}, ($44$) \\ 
  \textit{divorce} & \textit{wellness \& \dots}, ($41$) & \textit{weddings}, ($34$) & \textit{wellness \& \dots}, ($31$) & \textit{parents}, ($38$) \\ 
  \textit{education} & \textit{wellness \& \dots}, ($24$) & \textit{politics}, ($18$) & \textit{politics}, ($23$) & \textit{politics}, ($27$) \\ 
  \textit{entertainment} & \textit{wellness \& \dots}, ($135$) & \textit{politics}, ($44$) & \textit{politics}, ($41$) & \textit{politics}, ($90$) \\ 
  \textit{fifty} & \textit{wellness \& \dots}, ($63$) & \textit{wellness \& \dots}, ($62$) & \textit{wellness \& \dots}, ($49$) & \textit{wellness \& \dots}, ($74$) \\ 
  \textit{food, drink \dots} & \textit{wellness \& \dots}, ($125$) & \textit{wellness \& \dots}, ($44$) & \textit{wellness \& \dots}, ($64$) & \textit{wellness \& \dots}, ($63$) \\ 
  \textit{good news} & \textit{wellness \& \dots}, ($20$) & \textit{parents}, ($21$) & \textit{green \& \dots}, ($20$) & \textit{parents}, ($19$) \\ 
  \textit{green} \& \dots & \textit{wellness \& \dots}, ($74$) & \textit{politics}, ($42$) & \textit{politics}, ($42$) & \textit{politics}, ($52$) \\ 
  \textit{home} \& living & \textit{wellness \& \dots}, ($61$) & \textit{style \& beauty}, ($29$) & \textit{food, drink \dots}, ($18$) & \textit{food, \textit{drink} \dots}, ($21$) \\ 
  \textit{impact} & \textit{wellness \& \dots}, ($122$) & \textit{wellness \& \dots}, ($89$) & \textit{wellness \& \dots}, ($73$) & \textit{wellness \& \dots}, ($91$) \\ 
  \textit{latino voices} & \textit{politics}, ($29$) & \textit{politics}, ($22$) & \textit{entertainment}, ($17$) & \textit{politics}, ($41$) \\ 
  \textit{media} & \textit{politics}, ($78$) & \textit{politics}, ($91$) & \textit{politics}, ($61$) & \textit{politics}, ($114$) \\ 
  \textit{money} & \textit{wellness \& \dots}, ($34$) & \textit{business}, ($47$) & \textit{business}, ($32$) & \textit{business}, ($55$) \\ 
  \textit{parents} & \textit{wellness \& \dots}, ($221$) & \textit{wellness \& \dots}, ($117$) & \textit{wellness \& \dots}, ($130$) & \textit{wellness \& \dots}, ($137$) \\ 
  \textit{politics} & \textit{wellness \& \dots}, ($182$) & \textit{world news}, ($112$) & \textit{world news}, ($85$) & \textit{world news}, ($78$) \\ 
  \textit{queer voices} & \textit{politics}, ($47$) & \textit{entertainment}, ($42$) & \textit{politics}, ($55$) & \textit{politics}, ($68$) \\ 
  \textit{religion} & \textit{wellness \& \dots}, ($48$) & \textit{wellness \& \dots}, ($29$) & \textit{wellness \& \dots}, ($29$) & \textit{politics}, ($36$) \\ 
  \textit{science} & \textit{wellness \& \dots}, ($63$) & \textit{wellness \& \dots}, ($46$) & \textit{wellness \& \dots}, ($46$) & \textit{wellness \& \dots}, ($45$) \\ 
  \textit{sports} & \textit{wellness \& \dots}, ($66$) & \textit{entertainment}, ($27$) & \textit{entertainment}, ($23$) & \textit{entertainment}, ($27$) \\ 
  \textit{style \& beauty} & \textit{wellness \& \dots}, ($125$) & \textit{entertainment}, ($61$) & \textit{entertainment}, ($57$) & \textit{entertainment}, ($90$) \\ 
  \textit{tech} & \textit{wellness \& \dots}, ($29$) & \textit{business}, ($29$) & \textit{business}, ($23$) & \textit{business}, ($28$) \\ 
  \textit{travel} & \textit{wellness \& \dots}, ($141$) & \textit{food, drink \dots}, ($51$) & \textit{wellness \& \dots}, ($37$) & \textit{food, \textit{drink} \dots}, ($44$) \\ 
  \textit{weddings} & \textit{wellness \& \dots}, ($39$) & \textit{wellness \& \dots}, ($22$) & \textit{wellness \& \dots}, ($28$) & \textit{divorce}, ($27$) \\ 
  \textit{weird news} & \textit{wellness \& \dots}, ($33$) & \textit{travel}, ($18$) & \textit{green \& \dots}, ($18$) & \textit{crime}, ($27$) \\ 
  \textit{wellness} \& \dots & \textit{parents}, ($73$) & \textit{parents}, ($120$) & \textit{parents}, ($81$) & \textit{parents}, ($112$) \\ 
  \textit{women} & \textit{wellness \& \dots}, ($86$) & \textit{wellness \& \dots}, ($73$) & \textit{wellness \& \dots}, ($65$) & \textit{wellness \& \dots}, ($76$) \\ 
  \textit{world news} & \textit{politics}, ($92$) & \textit{politics}, ($62$) & \textit{politics}, ($81$) & \textit{politics}, ($114$) \\ 
   \hline
\end{tabular}
\caption{Kategorien, in die die meisten Beobachtungen klassifiziert wurden im Falle einer Fehlklassifikation für die Modelle der Endauswahl. In Klammern die zugehörige Anzahl der Beobachtungen.}
\label{tab:neighborClasses}
\end{table}





\begin{itemize}
\item vorstellung: struktur, hyperparameter, word embedding sizes und dimensions
    \item metriken wie in vorauswahl nur ausführlicher
    \item neighbor classes (mit 2 größten neighbor classes?)
    \item eingehen auf fehlklassifikationskosten
    \item accuracy by class comparison
    \item confidence vs accuracy plots
    \item maß wie sicher ist sich das Verfahren, wenn es die richtige klasse ist? compareProbVsAcc/CE
\end{itemize}{}

\subsection{Analyse der Modelle}

\begin{itemize}

    \item beobachtungen verändern, wörter wegnehmen, hinzufügen, reihenfolge ändern und schauen ob das verfahren stabil /sensitiv zur reihenfolge
    \item variablenwichtigkeit bei xgboost als wordcloud plotten, dalex und sonstiges
    \item random forest direkt variablenwichtigkeit
    \item nachbarklassen identifizieren durch confusionmatrix
    \item confusionmatrix zeigen
    \item convolutional filters holen und ähnlichkeiten zu combinationen aus word vectors erhalten über tupel/tripel von wortvectoren laufen lassen und schauen wo die ähnlichkeit am größten ist
\end{itemize}{}

\subsubsection{Anpassung des besten Modell auf den gesamten Datensatz}

\section{Zusammenfassung}

\subsection{Ergebnisse}

\begin{itemize}
    \item Bi-LSTM performt am besten
    \item welches Modell ist bzgl wenig rechenzeit und Speichergröße zu empfehlen?
\end{itemize}{}



\subsection{Fazit und Ausblick}

\begin{itemize}
    \item Weitere Architekturen im Bereich Bi-LSTM (Attention, sentence level etc)
    \item kompliziertere word embeddings wie encoder technologien wie BERT
\end{itemize}


\newpage

\end{document}